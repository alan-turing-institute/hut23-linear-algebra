\documentclass[10pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{beton}
\usepackage{eulervm}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{microtype}
%\usepackage[medium, compact]{titlesec}
\usepackage[inline]{asymptote}
\DeclareFontSeriesDefault[rm]{bf}{sbc}
% \usepackage{amssymb}
%% Turing grid is 21 columns (of 1cm if we are using A4)
%% Usually 4 "big columns", each of 4 text cols plus 1 gutter col;
%% plus an additional gutter on the left.
\usepackage[left=1cm, textwidth=11cm, marginparsep=1cm, marginparwidth=7cm]{geometry}
\usepackage[Ragged, size=footnote, shape=up]{sidenotesplus}
%% We used to use a two-column layout
% \setlength{\columnsep}{1cm}
\DeclareBoldMathCommand{\setR}{R}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\eg}{\emph{Example:}}
\newcommand{\ie}{\emph{i.e.}}
\hyphenation{anti-sym-met-ric}
%%
\author{James Geddes}
\date{\today}
\title{Linear Regression Done Right}
\begin{document}
\maketitle

This note is an attempt to rewrite chapter 9 of Deisenroth \emph{et
al.}, on linear regression.

\section*{Introduction}

Here are three problems which all seem to have something in common:

\begin{enumerate}
\item An ``urban farm'' grows crops underground in a vacant tunnel. To
  monitor environmental conditions, temperature sensors are placed at
  various locations around the tunnel. The farmers would like a sense
  of the temperature at arbitrary locations.
\item A data scientist is asked to create a model to predict the
  species of a penguin, given some facts about its weight, flipper
  size and bill size.
\item An economist wants to understand whether there is a relationship
  between measures of childhood education and later income.
\end{enumerate}

In each of these examples one might imagine that there is, in the
world, some sort of map, or function, from an “input” to an “output.”
In the underground farm, the input is “physical location” and the
output is “temperature;” for the data scientist, the input is “triple
of morphological measurements” and the output is “species;” for the
economist, the input is whatever measures of childhood education are
used, and the output is, perhaps, “income at age 30.”

In each case, there is given a set of observations, known as “the
data,” comprising particular inputs and their corresponding
outputs. The challenge is somehow to estimate, or approximate, or
model, the “true” relationship between inputs and outputs, so that the
model of the relationship matches the data, more or
less.\sidenote|-3in|{Why “more or less”? Why not match the data
  exactly? One reason, often cited, is “measurement error:” the idea
  that the $y_i$s are not measured exactly but contain some “noise”
  and so will differ from those generated by the real map. (Textbooks
  often harp on this reason, which seems odd to me: is there
  \emph{really} so much error in measuring, I don't know,
  temperature?)\sidepar%
  %
  A related but perhaps more plausible reason is that, in the real
  world, the outputs are not likely to be fully determined by the
  measured inputs. For example, income at age 30 is clearly not
  determined by education alone: multiple other inputs, many of which
  are difficult to measure, must play a role. If we were somehow able
  to obtain another measurement of $y$ for the very same input $x_i$,
  these other, hidden, inputs would presumably be different and we
  would not obtain the same~$y_i$.\sidepar%
  %
  Finally, it has been historical practice to consider only “simple”
  models, in order to make the calculations tractable. We might then
  expect that the “true” function---the one the real world is using to
  generate the data---will not be matched by our simplified model. For
  example, in econometrics or social science, it's not uncommon to fit
  a linear relationship, even when there is no reason to believe that
  the real world is linear. In this case what we're looking for is a
  function that is “close to” the real function and so only
  approximates the data.}

There are differences between these three problems and one might
wonder how significant they are. Can we capture the similarities
between these different problems in a way that will let us attempt to
find useful models?

To be more specific, suppose there is given a set, $X$, of possible
inputs, a set, $Y$, of possible outputs, and a collection of $d$ pairs
$(x_i, y_i)\in X\times Y$ (for $i=1,\dots,d$), that comprise the data. For
now, we suppose no extra structure on $X$; however, on $Y$ we imagine
there is some notion of “closeness” (to be made precise later). If
$y_1\in Y$ and $y_2\in Y$ are “close,” we write $y_1 \approx y_2$. (All of this
is somewhat informal.) Consider the challenge of finding a map,
$\hat{f}\colon X\to Y$, having the property that
$\hat{f}(x_i) \approx y_i$ for each~$i$.

One immediate snag is that finding such a map is \emph{far too
  easy}. Consider:
\begin{equation*}
  \hat{f}(x) =
  \begin{cases}
    y_i & \text{if $x = x_i$ for some $i$;} \\
      0 & \text{otherwise}.
  \end{cases}
\end{equation*}
This map is not just “close” to the data: it exactly matches the
data. However, since it is zero everywhere else, it seems implausible
that it represents the real world. What we presumably meant to ask for
was a function that agrees with the data \emph{and} is likely to agree
with the real function on \emph{other} values of the input, values we
haven't seen yet.

A possible response to this snag is to observe that this function,
$\hat{f}$, is somehow “physically unreasonable.” We just don't expect
the real world to behave as $\hat{f}$ does. On this view, what we
should do is identify a set of “reasonable” functions and then search
for $\hat{f}$ only within this set. In order to identify the set of
reasonable functions, one might appeal to a nomological condition: if
we had a \emph{theory} of the world, one which requires the function
to satisfy some differential equation, say, we might demand that our
approximating function also satisifies this law.\sidenote{More
  commonly, one tends to conflate reasonableness with pragmatism: one
  allows only functions that are “simple,” in some way, in order to
  make the problem tractable. “Simple” might mean “smooth,” or
  “low-order” or even “linear.”}

However we define “reasonable,” imagine that, somehow or other, we
have decided on a set of functions, $\mathcal{F}$, and what we are really asking
for is a function taken from \emph{this} set that is in some way
“close” to the data.\sidenote{It turns out to be quite difficult to
  get the “size” of $\mathcal{F}$ just right. If there are too few functions to
  draw from, we run the risk of not being able to match the real
  function; if there are too many, we run the risk of choosing one
  that is not physically reasonable just because it is a good match to
  the data.} In order to make any further progress we need to say
something more specific about the meaning of “close.”

\section{Least squares}

A popular version of “close” is as follows. First, suppose that the
set of “possible outputs,” $Y$, is the real numbers,~$\setR$. For any
particular datum, $(x_i, y_i)$, we might say that $y_i$ is close to
$f(x_i)$ just in case $( f(x)_i-y_i )$ is small. Extending this idea,
define a function on $\mathcal{F}$ by
\[
L(f) = \sum_{i=1}^d{(f(x_i) - y_i)}^2.
\]
This function seems to capture what we mean by a measure of
“closeness”. It is clearly related to the distance of each $y_i$ from
the $f(x_i)$; it is non-negative; and it is zero only when the
function exactly matches the data. A function like $L(f)$ is known as
the \emph{loss function}, and this one is sometimes called the
\emph{quadratic loss} or \emph{squared loss}.\sidenote{One might
  attempt to justify this choice of $L(f)$ from some statistical or
  other principles. In truth, I suspect it is popular largely because
  it makes the subsequent calculations tractable.}

The expression for $L(f)$ looks a lot like the (square of the)
Euclidean distance, in $\setR^d$, between the point
$(y_1, \dotsc, y_d)$, which we shall denote by $\bm{y}$, and the point
$(f(x_1), \dotsc, f(x_d))$, and this observation suggests a helpful
way to rewrite the loss function. The first point,
$(y_1, \dotsc, y_d)$, is the $y$-values of the data “packaged up” into
a single point in~$\setR^d$. The second point,
$(f(x_1), \dotsc, f(x_d))$, is, roughly, the result of “evaluating $f$
at the data, $x_i$.” The notion of evaluating $f$ at the data is
captured by the following map, knowm as the \emph{evaluation
  map},~$\mathcal{E}_{\bm{x}}$:\sidenote{The subscript $\bm{x}$ is there to
  remind ourselves that the evaluation map depends upon the $x$-values
  of the data. However, note that $\bm{x}$ is \emph{not}, in general,
  a vector, because $X$ is not, in general, a vector space. One is
  perfectly entitled to write, say, $\bm{x}=(x_1, \dotsc, x_d)$, but
  what is denoted is a tuple, not a vector.}
\[
  \begin{aligned}
    \mathcal{E}_{\bm{x}} \colon \mathcal{F} &\to \setR^d \\
    f &\mapsto (f(x_1), \dotsc, f(x_d)).
  \end{aligned}
\]

With this notation, we can say that the loss function is the squared
Euclidean distance, in $\setR^d$, between $\bm{y}$ and
$\mathcal{E}_{\bm{x}}(f)$.


. Recall that
the vector space $\setR^d$ is the set of tuples of real numbers, with
addition and scalar multiplication defined elementwise. We introduce
on $\setR^d$ a bilinear form, $\Delta$, as follows. For any vectors
$\bm{u} = (u_1,\dotsc,u_d)$ and $\bm{v}=(v_1,\dotsc,v_d)$, set
\[
  \Delta(\bm{u}, \bm{v}) = \sum_{i=1}^d u_iv_i.
\]
Note that for any $\bm{v}\in\setR^d$, the value of
$\Delta(\bm{v}, \bm{v})$ is the (square of the) Euclidean length of
$\bm{v}$, as computed by Pythogoras' rule. Likewise, the value of
$L(f)$ is the (square of the) Euclidean distance between the point
$(y_1, \dotsc, y_d)\in\setR^d$, which we shall denote by $\bm{y}$, and
the point $(f(x_1), \dotsc, f(x_d))$. This latter point is
Thus we have,
\begin{equation}
  L(f) = \Delta(\mathcal{E}_{\bm{x}}(f) - \bm{y}, \mathcal{E}_{\bm{x}}(f) - \bm{y}).
\end{equation}

Here is a summary of the discussion to this point. Our problem is to
choose, from a set of functions, $\mathcal{F}$, a particular function,
$\hat{f}$, which approximates the data in the sense that the values of
the function evaluated at the $x$-values of the data are “close to”
the $y$-values of the data. The concept of “close to” was chosen to
mean “having a small distance in the space $\setR^d$,” where
“distance” is the usual Euclidean distance computed by Pythagoras'
rule. In summary, we are to solve the following minimisation problem:
\begin{equation}
  \label{eq:least-squares-loss}
  \hat{f} = \argmin_{f\in\mathcal{F}} \Delta(\mathcal{E}_{\bm{x}}(f) - \bm{y}, \mathcal{E}_{\bm{x}}(f) - \bm{y}),
\end{equation}
where, in this minimisation, the data are held fixed.







\end{document}

\section*{Notes on the original text}

Like most machine learning techniques, linear regression involves the
representation of a particular real-world problem by mathematical
objects, as well as the use of mathematical methods to solve the
problem. Therefore, in writing an exposition of the subject, one will
move between informal descriptions and formal mathematics; between the
real world and the mathematical world.

It is important that the reader is clear at each point which is which!
Mathematically-inclined texts often conflate informal descriptions and
formal definitions, either by failing to give a proper definition when
one was presaged, or by dropping into heavy mathematics before
clarifying the question. 

For example, Deisenroth \emph{et al.}\ begin as
follows.\sidenote{There is one other sentence before this but it
merely introduces the chapter. I've numbered the sentences for ease of
reference; these numbers are not part of the original text. And I have
omitted one other sentence that is not important here.}

\begin{quote}
 [1] In \emph{regression}, we aim to find a function $f$ that maps
 inputs $\bm{x} \in \setR^D$ to corresponding function values $f(\bm{x})
 \in\setR$. [2] We assume that we are given a set of training inputs
 $\bm{x}_n$ and corresponding noisy observations $y_n = f(\bm{x}_n) +
 \epsilon$, where $\epsilon$ is an i.i.d.\ random variable that describes
 measurement/observation noise and potentially unmodeled processes
 [...]. [3] Our task is to find a function that not only models the
 training data, but generalizes well [...].
\end{quote}



\end{document}
