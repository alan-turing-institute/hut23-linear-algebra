\documentclass[10pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{beton}
\usepackage{eulervm}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{microtype}
%\usepackage[medium, compact]{titlesec}
\usepackage[inline]{asymptote}
\DeclareFontSeriesDefault[rm]{bf}{sbc}
% \usepackage{amssymb}
%% Turing grid is 21 columns (of 1cm if we are using A4)
%% Usually 4 "big columns", each of 4 text cols plus 1 gutter col;
%% plus an additional gutter on the left.
\usepackage[left=1cm, textwidth=11cm, marginparsep=1cm, marginparwidth=7cm]{geometry}
\usepackage[Ragged, size=footnote, shape=up]{sidenotesplus}
%% We used to use a two-column layout
% \setlength{\columnsep}{1cm}
\DeclareBoldMathCommand{\setR}{R}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\eg}{\emph{Example:}}
\newcommand{\ie}{\emph{i.e.}}
\hyphenation{anti-sym-met-ric}
%%
\author{James Geddes}
\date{\today}
\title{Linear Regression Done Right}
\begin{document}
\maketitle

This note is an attempt to rewrite chapter 9 of Deisenroth \emph{et
al.}, on linear regression.

\section*{Introduction}

Here are three problems which all seem to have something in common:

\begin{enumerate}
\item An ``urban farm'' grows crops underground in a vacant tunnel. To
  monitor environmental conditions, temperature sensors are placed at
  various locations around the tunnel. The farmers would like a sense
  of the temperature at arbitrary locations.
\item A data scientist is asked to create a model to predict the
  species of a penguin, given some facts about its weight, flipper
  size and bill size.
\item An economist wants to understand whether there is a relationship
  between measures of childhood education and later income.
\end{enumerate}

In each of these examples one might imagine that there is, in the
world, some sort of map, or function, from an “input” to an “output.”
In the underground farm, the input is “physical location” and the
output is “temperature;” for the data scientist, the input is “triple
of morphological measurements” and the output is “species;” for the
economist, the input is whatever measures of childhood education are
used, and the output is, perhaps, “income at age 30.”

In each case, there is given a set of observations, known as “the
data,” of particular inputs and their corresponding outputs. The
challenge is somehow to estimate, or approximate, or model the “true”
relationship between inputs and outputs so that the model of the
relationship matches the data, more or less.\sidenote|-3in|{Why “more
  or less”? Why not match the data exactly? One reason, often cited,
  is “measurement error:” the idea that the $y_i$s are not measured
  exactly but contain some “noise” and so will differ from those
  generated by the real map. (Textbooks often harp on this
  reason, which seems odd to me: is there \emph{really} so much error
  in measuring, I don't know, temperature?)\sidepar%
  %
  A related but perhaps more plausible reason is that, in the real
  world, the outputs are not likely to be fully determined by the
  measured inputs. For example, income at age 30 is clearly not
  determined by education alone: multiple other inputs, many of which
  are difficult to measure, must play a role. If we were somehow able
  to obtain another measurement of $y$ for the very same input $x_i$,
  these other, hidden, inputs would presumably be different and we
  would not obtain the same~$y_i$.\sidepar%
  %
  Finally, it has been historical practice to consider only “simple”
  models, in order to make the calculations tractable. We might then
  expect that the “true” function---the one the real world is using to
  generate the data---will not be matched by our simplified model. For
  example, in econometrics or social science, it's not uncommon to fit
  a linear relationship, even when there is no reason to believe that
  the real world is linear. In this case what we're looking for is a
  function that is “close to” the real function and so only
  approximates the data.}

There are differences between these three problems and one might
wonder how significant they are. Can we capture the similarities
between these different problems in a way that will let us attempt to
find useful models?

To be more specific, suppose there is given a set, $X$, of possible
inputs, a set, $Y$, of possible outputs, and a collection of $d$ pairs
$(x_i, y_i)\in X\times Y$ (for $i=1,\dots,d$), that comprise the data. For
now, we suppose no extra structure on $X$; however, on $Y$ we imagine
there is some notion of “closeness” (to be made precise later). If
$y_1\in Y$ and $y_2\in Y$ are “close,” we write $y_1 \approx y_2$. (All of this
is somewhat informal.) Consider the challenge of finding a map,
$\hat{f}\colon X\to Y$, having the property that
$\hat{f}(x_i) \approx y_i$ for each~$i$.

One immediate snag is that finding such a map is \emph{far too
  easy}. Consider:
\begin{equation*}
  \hat{f}(x) =
  \begin{cases}
    y_i & \text{if $x = x_i$ for some $i$;} \\
      0 & \text{otherwise}.
  \end{cases}
\end{equation*}
This map is not just “close” to the data: it exactly matches the
data. However, since it is zero everywhere else, it seems implausible
that it represents the “real world.” What we presumably meant to ask
for was a function that agrees with the data \emph{and} is likely to
agree with the real function on \emph{other} values of the input,
values we haven't seen yet.

A possible response to this snag is to observe that this function,
$\hat{f}$, is somehow “physically unreasonable.” We just don't expect
the real world to behave as $\hat{f}$ does. On this view, what we
should do is identify a set of “reasonable” functions and then search
only within this set. In order to identify the set of reasonable
functions, one might appeal to a nomological condition: if we had a
\emph{theory} of the world, one which requires the function to satisfy
some differential equation, say, we might demand that our
approximating function also satisifies this law.\sidenote{More
  commonly, one tends to conflate reasonableness with pragmatism: one
  allows only functions that are “simple,” in some way, in order to
  make the problem tractable. “Simple” might mean “smooth,” or
  “low-order” or even “linear.”}

At any rate, imagine that, somehow or other, we have decided on a set
of “reasonable” functions, $\mathcal{F}$, and what we are really asking for is a
function taken from \emph{this} set that is in some way “close” to the
data.\sidenote{It turns out to be quite difficult to get the “size” of
  $\mathcal{F}$ just right. If there are too few functions to draw from, we run
  the risk of not being able to match the real function; if there are
  too many, we run the risk of choosing one that is not physically
  reasonable just because it is a good match to the data.} In order to
make any further progress we need to say something more specific about
the meaning of “close.”

\section{Least squares}

Suppose, now, that $Y$, the set of “possible outputs,” is the real
numbers,~$\setR$. A popular measure of the closeness of a function,
$f\colon X\to\setR$, to the data, $(x_i, y_i)$, is the value of the expression
\[
\sum_{i=1}^d{(f(x_i) - y_i)}^2.
\]
This particular measure has several nice properties: it is very
clearly related to the distance of $y_i$ from $f(x_i)$; it is
non-negative; and it is zero only when the function exactly matches
the data.

There is a suggestive way to rewrite this expression. The vector space
$\setR^d$ is the set of tuples of real numbers, with addition and
scalar multiplication defined elementwise. We introduce on $\setR^d$ a
bilinear form, $\Delta$, as follows. For any vectors
$\bm{u} = (u_1,\dotsc,u_d)$ and $\bm{v}=(v_1,\dotsc,v_d)$ set
\[
\Delta(\bm{u}, \bm{v}) = \sum_{i=1}^d u_iv_i.
\]


Thus,
the problem of ordinary least-squares regression is to find
\[
  \hat{f} = \argmin_{f\in\mathcal{F}} \sum_{i=1}^N {(f(x_i) - y_i)}^2.
\]

The right hand side is, for each $f\in$



In general this problem is hard. Deep learning, for example, falls
under the very broad description above. In \emph{linear regression} we
make two choices that turn out to simplify the problem enormously.
First, we assume that the space of “outputs” is the real numbers, $Y =
\setR$ and so $\mathcal{F}$ consists of real-valued functions. Given two
real-valued functions there is a natural notion of their linear
combination: for two functions $f$ and $g$, and number $\alpha$, define the
function $f+\alpha g$ by:
\begin{equation*}
  (f+\alpha g)(x) = f(x) + \alpha g(x).
\end{equation*}
In other words, functions are added by “adding their values at each
point.” Our second choice is then to demand that, under this
definition of addition of functions and multiplication of functions by
nunbers, the space $\mathcal{F}$ is a vector space.\sidenote{Note that we have
\emph{not} said anything about the “input” space,~$X$; especially not
that it is a vector space! Nor have we specified what particular kinds
of functions make up $\mathcal{F}$, for example, we have not said that they must
be “linear functions of $X$.”}








\end{document}

\section*{Notes on the original text}

Like most machine learning techniques, linear regression involves the
representation of a particular real-world problem by mathematical
objects, as well as the use of mathematical methods to solve the
problem. Therefore, in writing an exposition of the subject, one will
move between informal descriptions and formal mathematics; between the
real world and the mathematical world.

It is important that the reader is clear at each point which is which!
Mathematically-inclined texts often conflate informal descriptions and
formal definitions, either by failing to give a proper definition when
one was presaged, or by dropping into heavy mathematics before
clarifying the question. 

For example, Deisenroth \emph{et al.}\ begin as
follows.\sidenote{There is one other sentence before this but it
merely introduces the chapter. I've numbered the sentences for ease of
reference; these numbers are not part of the original text. And I have
omitted one other sentence that is not important here.}

\begin{quote}
 [1] In \emph{regression}, we aim to find a function $f$ that maps
 inputs $\bm{x} \in \setR^D$ to corresponding function values $f(\bm{x})
 \in\setR$. [2] We assume that we are given a set of training inputs
 $\bm{x}_n$ and corresponding noisy observations $y_n = f(\bm{x}_n) +
 \epsilon$, where $\epsilon$ is an i.i.d.\ random variable that describes
 measurement/observation noise and potentially unmodeled processes
 [...]. [3] Our task is to find a function that not only models the
 training data, but generalizes well [...].
\end{quote}



\end{document}
