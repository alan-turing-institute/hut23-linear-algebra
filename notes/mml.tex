\documentclass[10pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{beton}
\usepackage{eulervm}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{microtype}
%\usepackage[medium, compact]{titlesec}
\usepackage[inline]{asymptote}
\DeclareFontSeriesDefault[rm]{bf}{sbc}
% \usepackage{amssymb}
%% Turing grid is 21 columns (of 1cm if we are using A4)
%% Usually 4 "big columns", each of 4 text cols plus 1 gutter col;
%% plus an additional gutter on the left.
\usepackage[left=1cm, textwidth=11cm, marginparsep=1cm, marginparwidth=7cm]{geometry}
\usepackage[Ragged, size=footnote, shape=up]{sidenotesplus}
%% We used to use a two-column layout
% \setlength{\columnsep}{1cm}
\DeclareBoldMathCommand{\setR}{R}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\eg}{\emph{Example:}}
\newcommand{\ie}{\emph{i.e.}}
\hyphenation{anti-sym-met-ric}
%%
\author{James Geddes}
\date{\today}
\title{Linear Regression Done Right}
\begin{document}
\maketitle

This note is an attempt to rewrite chapter 9 of Deisenroth \emph{et
al.}, on linear regression.

\section*{Introduction}

Here are three problems which all seem to have something in common:

\begin{enumerate}
\item An ``urban farm'' grows crops underground in a vacant tunnel. To
  monitor environmental conditions, temperature sensors are placed at
  various locations around the tunnel. The farmers would like a sense
  of the temperature at arbitrary locations.
\item A data scientist is asked to create a model to predict the
  species of a penguin, given some facts about its weight, flipper
  size and bill size.
\item An economist wants to understand whether there is a relationship
  between measures of childhood education and later income.
\end{enumerate}

In each of these examples one might imagine that there is, in the
world, some sort of map, or function, from an “input” to an “output.”
In the underground farm, the input is “physical location” and the
output is “temperature;” for the data scientist, the input is “triple
of morphological measurements” and the output is “species;” for the
economist, the input is whatever measures of childhood education are
used, and the output is, perhaps, “income at age 30.”

In each case, there is given a set of observations, known as “the
data,” of particular inputs and their corresponding outputs. The
challenge is somehow to estimate, or approximate, or model the “true”
relationship between inputs and outputs so that the model of the
relationship matches the data, more or less.\sidenote|-3in|{Why “more
  or less”? Why not match the data exactly? One reason, often cited,
  is “measurement error:” the idea that the $y_i$s are not measured
  exactly but contain some “noise” and so will differ from those
  generated by the real map. (Textbooks often harp on this
  reason, which seems odd to me: is there \emph{really} so much error
  in measuring, I don't know, temperature?)\sidepar%
  %
  A related but perhaps more plausible reason is that, in the real
  world, the outputs are not likely to be fully determined by the
  measured inputs. For example, income at age 30 is clearly not
  determined by education alone: multiple other inputs, many of which
  are difficult to measure, must play a role. If we were somehow able
  to obtain another measurement of $y$ for the very same input $x_i$,
  these other, hidden, inputs would presumably be different and we
  would not obtain the same~$y_i$.\sidepar%
  %
  Finally, it has been historical practice to consider only “simple”
  models, in order to make the calculations tractable. We might then
  expect that the “true” function---the one the real world is using to
  generate the data---will not be matched by our simplified model. For
  example, in econometrics or social science, it's not uncommon to fit
  a linear relationship, even when there is no reason to believe that
  the real world is linear. In this case what we're looking for is a
  function that is “close to” the real function and so only
  approximates the data.}

There are differences between these three problems and one might
wonder how significant they are. Can we capture the similarities
between these different problems in a way that will let us attempt to
find useful models?

To be more specific, suppose there is given a set, $X$, of possible
inputs, a set, $Y$, of possible outputs, and a collection of $d$ pairs
$(x_i, y_i)\in X\times Y$ (for $i=1,\dots,d$), that comprise the data. For
now, we suppose no extra structure on $X$; however, on $Y$ we imagine
there is some notion of “closeness” (to be made precise later). If
$y_1\in Y$ and $y_2\in Y$ are “close,” we write $y_1 \approx y_2$. (All of this
is somewhat informal.) Consider the challenge of finding a map,
$\hat{f}\colon X\to Y$, having the property that
$\hat{f}(x_i) \approx y_i$ for each~$i$.

One immediate snag is that finding such a map is \emph{far too
  easy}. Consider:
\begin{equation*}
  \hat{f}(x) =
  \begin{cases}
    y_i & \text{if $x = x_i$ for some $i$;} \\
      0 & \text{otherwise}.
  \end{cases}
\end{equation*}
This map is not just “close” to the data: it exactly matches the
data. However, since it is zero everywhere else, it seems implausible
that it represents the “real world.” What we presumably meant to ask
for was a function that agrees with the data \emph{and} is likely to
agree with the real function on \emph{other} values of the input,
values we haven't seen yet.

A possible response to this snag is to observe that this function,
$\hat{f}$, is somehow “physically unreasonable.” We just don't expect
the real world to behave as $\hat{f}$ does. On this view, what we
should do is identify a set of “reasonable” functions and then search
only within this set. In order to identify the set of reasonable
functions, one might appeal to a nomological condition: if we had a
\emph{theory} of the world, one which requires the function to satisfy
some differential equation, say, we might demand that our
approximating function also satisifies this law.\sidenote{More
  commonly, one tends to conflate reasonableness with pragmatism: one
  allows only functions that are “simple,” in some way, in order to
  make the problem tractable. “Simple” might mean “smooth,” or
  “low-order” or even “linear.”}

At any rate, imagine that, somehow or other, we have decided on a set
of “reasonable” functions, $\mathcal{F}$, and what we are really asking for is a
function taken from \emph{this} set that is in some way “close” to the
data.\sidenote{It turns out to be quite difficult to get the “size” of
  $\mathcal{F}$ just right. If there are too few functions to draw from, we run
  the risk of not being able to match the real function; if there are
  too many, we run the risk of choosing one that is not physically
  reasonable just because it is a good match to the data.} In order to
make any further progress we need to say something more specific about
the meaning of “close.”

\section{Least squares}

Suppose, now, that the set of “possible outputs,” $Y$, is the real
numbers,~$\setR$. A popular measure of the closeness of a function,
$f\colon X\to\setR$, to the data, $(x_i, y_i)$, is the value of the
expression $\sum_{i=1}^d{(f(x_i) - y_i)}^2$. This particular expression
has several nice properties: it is very clearly related to the
distance of the $y_i$ from the $f(x_i)$; it is non-negative; and it is
zero only when the function exactly matches the data. Holding the data
fixed, one thinks of this expression as a real-valued function on
$\mathcal{F}$, sometimes called the \emph{loss function}, and denoted
$L(f)$. Thus:
\[
L(f) = \sum_{i=1}^d{(f(x_i) - y_i)}^2.
\]
What we mean by “the function is close to the data” is now more
precisely the proposition that $L(f)$ is small.

There is a suggestive way to rewrite the loss function. Recall that
the vector space $\setR^d$ is the set of tuples of real numbers, with
addition and scalar multiplication defined elementwise. We introduce
on $\setR^d$ a bilinear form, $\Delta$, as follows. For any vectors
$\bm{u} = (u_1,\dotsc,u_d)$ and $\bm{v}=(v_1,\dotsc,v_d)$ set
\[
\Delta(\bm{u}, \bm{v}) = \sum_{i=1}^d u_iv_i.
\]

We now express the loss function using this bilinear form. To do so,
we need an element of $\setR^d$ corresponding to the $y_i$, and an
element of $\setR^d$ corresponding to the~$f(x_i)$. For the former,
denote by $\bm{y}$ the element of $\setR^d$ obtained by packaging up
the $y$-values of the data into a tuple, $\bm{y}=(y_1, \dotsc,
y_d)$. For the latter, note that we obain $(f(x_1),\dotsc,f(x_d)$, an
element of $\setR^d$, by “evaluating $f$ at the data.” We can capture
this idea as a map, known as the \emph{evaluation
  map},~$\mathcal{E}_{\bm{x}}$:\sidenote{The subscript $\bm{x}$ is there to
  remind ourselves that the evaluation map depends upon the $x$-values
  of the data. However, note that $\bm{x}$ is \emph{not}, in general,
  a vector, because $X$ is not, in general, a vector space. One is
  perfectly entitled to write, say, $\bm{x}=(x_1, \dotsc, x_d)$, but
  what is denoted is a tuple, not a vector.}
\[
  \begin{aligned}
    \mathcal{E}_{\bm{x}} \colon \mathcal{F} &\to \setR^d \\
    f &\mapsto (f(x_1), \dotsc, f(x_d)).
  \end{aligned}
\]
Thus we have,
\[
  L(f) = \Delta(\mathcal{E}_{\bm{x}}(f) - \bm{y}, \mathcal{E}_{\bm{x}}(f) - \bm{y}).
\]

It is worth summarising the preceding discussion. Our problem is to
choose, from a set of functions, $\mathcal{F}$, a particular function,
$\hat{f}$, which approximates the data in the sense that the values of
the function evaluated at the $x$-values of the data are “close to”
the $y$-values of the data. The concept of “close to” was chosen to
mean “having a small distance in the space $\setR^d$,” where
“distance” is the usual Euclidean distance computed by Pythagoras'
rule. In summary, we are to solve the following minimisation problem:
\begin{equation}
  \label{eq:least-squares-loss}
  \hat{f} = \argmin_{f\in\mathcal{F}} \Delta(\mathcal{E}_{\bm{x}}(f) - \bm{y}, \mathcal{E}_{\bm{x}}(f) - \bm{y}),
\end{equation}
where, in this minimisation, the data are held fixed.







\end{document}

\section*{Notes on the original text}

Like most machine learning techniques, linear regression involves the
representation of a particular real-world problem by mathematical
objects, as well as the use of mathematical methods to solve the
problem. Therefore, in writing an exposition of the subject, one will
move between informal descriptions and formal mathematics; between the
real world and the mathematical world.

It is important that the reader is clear at each point which is which!
Mathematically-inclined texts often conflate informal descriptions and
formal definitions, either by failing to give a proper definition when
one was presaged, or by dropping into heavy mathematics before
clarifying the question. 

For example, Deisenroth \emph{et al.}\ begin as
follows.\sidenote{There is one other sentence before this but it
merely introduces the chapter. I've numbered the sentences for ease of
reference; these numbers are not part of the original text. And I have
omitted one other sentence that is not important here.}

\begin{quote}
 [1] In \emph{regression}, we aim to find a function $f$ that maps
 inputs $\bm{x} \in \setR^D$ to corresponding function values $f(\bm{x})
 \in\setR$. [2] We assume that we are given a set of training inputs
 $\bm{x}_n$ and corresponding noisy observations $y_n = f(\bm{x}_n) +
 \epsilon$, where $\epsilon$ is an i.i.d.\ random variable that describes
 measurement/observation noise and potentially unmodeled processes
 [...]. [3] Our task is to find a function that not only models the
 training data, but generalizes well [...].
\end{quote}



\end{document}
