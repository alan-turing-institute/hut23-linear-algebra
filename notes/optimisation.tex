\documentclass[10pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{beton}
\usepackage{eulervm}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{microtype}
%\usepackage[medium, compact]{titlesec}
\usepackage[inline]{asymptote}
\DeclareFontSeriesDefault[rm]{bf}{sbc}
% \usepackage{amssymb}
%% Turing grid is 21 columns (of 1cm if we are using A4)
%% Usually 4 "big columns", each of 4 text cols plus 1 gutter col;
%% plus an additional gutter on the left.
\usepackage[left=1cm, textwidth=11cm, marginparsep=1cm, marginparwidth=7cm]{geometry}
\usepackage[Ragged, size=footnote, shape=up]{sidenotesplus}
%% We used to use a two-column layout
% \setlength{\columnsep}{1cm}
\title{Simple optimisation (on vector spaces)}
\author{James Geddes}
\date{\today}
%%
\DeclareBoldMathCommand{\setR}{R}
\DeclareBoldMathCommand{\bfC}{C}
\newcommand{\bzero}{\mathbold{0}} % I don't know why \bm{0} fails.
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\eg}{\emph{Example:}}
\newcommand{\ie}{\emph{i.e.}}
\hyphenation{anti-sym-met-ric}
\begin{document}
\maketitle

A classic problem is that of finding the location of the minimum of
some real-valued function. It is a hard problem! It is not just that
the function itself might be complicated in some way; the
\emph{domain} of the function might be complicated: it could be
discrete, or high-dimensional, or have some complicated shape. All of
these can prevent an analytic solution and impede a numerical one.

The general problem is this. Let $X$ be a set, possibly with
additional structure, and $f\colon X \to \setR$ a real-valued function
on~$X$. We are to find $x_\text{min}\in X$ (if one exists) such that
\[
 f(x_\text{min}) \leq f(x) \quad\text{for all $x\in X$}.  
\]
The value $x_\text{min}$ is called the \emph{minimiser}
of~$f$. Alternatively, we say
\begin{equation}
  x_\text{min} = \argmin_{x\in X} f(x).
\label{eq:argmin}
\end{equation}

Sometimes, the domain of the function is, or can be approximated by, a
vector space; and then the problem may be more tractable. To get a
sense for the problem in this case, suppose, for the moment, that the
domain of $f$ is the reals. Here are a few, simple examples where the
minimum can be found without much difficulty.

\eg{} A constant function, $f(x) = a$. Such a function attains its
minimum value everywhere. To say it another way, there is no unique
point at which it is a minimum.

\eg{} A linear function,\sidenote{There is a slight inconsistency in
  terminology here.  A linear \emph{map} from one vector space to
  another necessarily maps zero to zero, whereas a linear function,
  like this one, may have a constant term.} $f(x) = a + bx$. This
function has no minimum (assuming $b\neq0$). Its value can be made
arbitrarily negative by taking $x$ sufficiently large.

\eg{} A quadratic, $f(x) = a + bx + cx^2$. The previous examples could
be solved by inspection. One way to approach the general case is to
observe that, at any minimum of a function, the derivative of the
function must be zero and its second derivative must be strictly
positive.\sidenote{Other values of the second derivative indicate
  maxima ($f''(x) < 0$) or “points of inflection” ($f''(x)=0$).}
However, for the case of a quadratic, there is, as an alternative, a
well-known “trick,” which, as a bonus, also demonstrates that a
mimumum is a global minimum.

Suppose we can find numbers $\kappa$, $\gamma$, and $\xi$ such that
\begin{equation}
  a + bx + cx^2 = \kappa + \gamma{(x - \xi)}^2.
\label{eq:completing-the-square}
\end{equation}
Since adding a constant to a function does not change the
\emph{location} of any mimimum, we have only to find the minimiser of
$\gamma{(x-\xi)}^2$. And, since ${(x-\xi)}^2$ is always non-negative, and zero
only when $x-\xi$, a minimum will exist only when $\gamma >0$, and that
minimum will be at $x=\xi$.
\begin{marginfigure}
  \begin{center}
    \asyinclude[width=4cm, height=4cm, keepAspect=false]{quadr.asy}
  \end{center}
\caption{A graph of $f(x) = 9 - 8x + 2x^2$. The minimum occurs at
  $x=2$, which may be seen by rewriting $f$ as $f(x) =
  2{(x-2)}^2+1$.\label{fig:quadratic}}
\end{marginfigure}
The values of $\kappa$, $\gamma$, and $\xi$ can be read off from the equation
above: equating terms in $x^2$ gives $\gamma=c$ and equating terms in
$x$ gives $\xi=-\frac{1}{2}c^{-1}b$. (The value of $\kappa$ does not affect
the location of the minimum but for completeness it is
$\kappa=a-\frac{1}{4}cb^2$.) This trick is known as “completing the
square.”

We return, now, to the multidimensional case. Fix, once and for all, a
finite-dimensional, real vector space, $V$, not necessarily
one-dimensional. What is the equivalent, on $V$, of a quadratic
function on~$\setR$? We would like to write something of the form,
“$f(v) = a+bv+cv^2$” but in order to do so we must attach a meaning to
each of the terms. The issue is that $f(v)$ is a real number; whereas,
“$bv$”, for instance, is on the face of it a vector; and “$cv^2$” is
not even defined.

To make sense of the linear term, we must find some way of forming a
number from a vector, while “respecting the vector space structure of
$V$.” An obvious candidate for such a map is an element of $\mathcal{L}(V,
\setR)$. This space, $\mathcal{L}(V, \setR)$, itself has the structure of a
vector space. It is known as the \emph{dual} of $V$ and is
denoted~$V^*$. A term that is “linear in $v$” is therefore an
expression of the form $\tilde{b}(v)$ for some $\tilde{b}\in V^*$.
\marginpar{The vector space structure on $V^*$ is as follows. For
  $\tilde{p}, \tilde{q}\in V^*$ and $\alpha\in\setR$, the linear combination
  $\tilde{p}+\alpha\cdot\tilde{q}$ is that element of $V^*$ whose action on any
  vector $x\in V$ is $(\tilde{p}+\alpha\cdot\tilde{q})(x) = \tilde{p}(x) +
  \alpha\tilde{q}(x)$. \sidepar

  Suppose $(e_1, \dotsc, e_n)$ is a basis for~$V$. To define an
  element of $V^*$ it suffices to give its action on a basis
  of~$V$. Define $\tilde{e}^j\in V^*$ by
  \[
      \tilde{e}^j(e_i) = 
      \begin{cases}
        1 & \text{if $i=j$}, \\
        0 & \text{otherwise.}
      \end{cases}
  \]
  These $\tilde{e}^j$ are a basis for~$V^*$, known as the dual
  basis. Note that the dimension of $V^*$ is the same as that of $V$
  (in the finite-dimensional case). \sidepar

  \hrule
}

It is less clear what might be meant by a “quadratic term.” It would
seem to involve the “product of a vector with itself,” and that is not
an operation that is obviously available in a general vector
space. Once again, we make use of the dual space. The idea is to start
with $v$, somehow “carry it across” to $V^*$, and then act with the
result on~$w$.

Thus, let $\bfC\colon V\to V^*$ be a linear map from $V$ to its
dual. For any vector $v\in V$, we obtain $\bfC(v)\in V^*$ (see
figure~\ref{fig:bilinear-form}). Since an element of $V^*$ is a linear
map from $V$ to~$\setR$, we may apply $\bfC(v)$ to $w\in V$ and thereby
obtain a number, $(\bfC(v))(w)$.

\begin{marginfigure}
  \begin{center}
    \asyinclude[width=5cm]{bilinear-form.asy}
  \end{center}
  \caption{A vector space $V$ and its dual $V^*$, showing: an element
    $v\in V$; a linear map $\bfC\colon V\to V^*$; and the image of $v$ in
    $V^*$ under $\bfC$.\label{fig:bilinear-form}}
\end{marginfigure}
In a sense, one thinks of $\bfC$ as a map from pairs
$(v,w)\in V\times V$ to the reals, which is “linear in both $v$ and
$w$.” This view suggests a less cumbersome notation: instead of
$(\bfC(v))(w)$ we shall write $\bfC(v,w)$. Thus, by $\bfC(v, w)$ we
shall mean, “apply $\bfC$ to $v$, obtaining an element of $V^*$, and
apply this element to $w$, obtaining a number.” When $\bfC$ is viewed
from this perspective, it is known as a \emph{bilinear form}.

\eg{} For $\bfC$ any bilinear form,
$\bfC(\alpha v, \beta w) = \alpha\beta \bfC(v,w)$ (which very much gives
$\bfC$ the flavour of a product).

We can now say roughly what is meant by a “quadratic term:” it is an
expression of the form $\bfC(v,v)$ for some bilinear form~$\bfC$.

Notice, however, that in this expression $\bfC$ is applied to a single
$v$ (twice); whereas more generally a bilinear form may be applied to
two different vectors. Is there some redundancy in this definition?
Let $\bm{A}$ be any bilinear form such that $\bm{A}(v,w)=-\bm{A}(w,v)$ and consider
the bilinear form $\bfC+\bm{A}$. By linearity, we have $(\bfC+\bm{A})(v, v) =
\bfC(v,v)+\bm{A}(v,v)$. However, $\bm{A}(v,v)=-\bm{A}(v,v)$ (by assumption), whence
$\bm{A}(v,v)=0$. Thus $(\bfC+\bm{A})(v,v)=\bfC(v,v)$; that is, $\bfC+\bm{A}$ gives rise to the
same quadratic form as~$\bfC$.

A bilinear form $\bm{A}$ for which $\bm{A}(v,w)=-\bm{A}(w,v)$ is said
to be \emph{antisymmetric}. Conversely, a bilinear form $\bm{S}$ for
which $\bm{S}(v,w)=\bm{S}(w,v)$ is said to be \emph{symmetric}. From
the foregoing, the addition, to $\bfC$, of any antisymmetric bilinear
form does not change value of $\bfC(v,v)$. Now consider the identity
(for any bilinear form):
\[
  \bfC(v, w) = \frac{1}{2}\bigl[\bfC(v,w) + \bfC(w,v)\bigr]
  + \frac{1}{2}\bigl[\bfC(v,w) - \bfC(w,v)\bigr].
\]
The first term on the right-hand side is symmetric whereas the second
is antisymmetric. Since the antisymmetric term vanishes when both
arguments are the same, we assume, without loss of generality, that
$\bfC$ is symmetric.\sidenote{The identity
  \[
    \bfC(v, w) = \frac{1}{2}\bfC(v+w, v+w) - \bfC(v,v) -\bfC(w,w),
  \]
  known as the \emph{polarisation identity}, shows that the symmetric
  form may be recovered from $\bfC(v,v)$.}

We are now in a position to say what we mean by a quadratic function on
a vector space. It is a function of the form:
\begin{equation}
  f(v) = a - 2\tilde{b}(v) + \bfC(v, v),
  \label{eq:quadratic-function}
\end{equation}
where, on the right-hand side, $a$ is a number, $\tilde{b}$ is an element
of the dual space and $\bfC$ is a symmetric bilinear form. (The factor of
$-2$ is conventional as it simplifies certain calculations.)

Having written down a function on $V$, we return to the problem of
finding the location of its minimum.

By direct analogy with the one-dimensional case,
eq.~\eqref{eq:completing-the-square}, we might attempt to rewrite
eq.~\eqref{eq:quadratic-function} as:
\begin{equation}
  a - 2\tilde{b}(v) + \bfC(v, v) = \kappa + \bm{\Gamma}(v - \xi, v - \xi),
  \label{eq:vector-square}
\end{equation}
where now $\kappa$ is a number, $\xi$ is a vector (which we hope will turn
out to be the minimiser of $f$!), and $\bm{\Gamma}$ is a symmetric bilinear
form. (Note that, previously, the last term on the right-hand side
involved the expression ${(x-\xi)}^2$; here, a symmetric bilinear form
is required to effect the square.)

In the one-dimensional case we next expanded the term in ${(x-\xi)}^2$
and equated coefficients of each power of $x$. To do the same thing
here, we shall have to expand the term in $\bm{\Gamma}$. Recall the meaning of
$\bm{\Gamma}(v-\xi, v-\xi)$: $\bm{\Gamma}$ is applied to $v-\xi$ to obtain an element
of~$V^*$; this element is then applied to $v-\xi$. Both of these
applications are linear, and so
\[
  \begin{aligned}
  \bm{\Gamma}(v-\xi,v-\xi) & = \bm{\Gamma}(v-\xi,v)-\bm{\Gamma}(v-\xi, \xi) \\
  & = \bm{\Gamma}(v,v)-\bm{\Gamma}(v, \xi) - \bm{\Gamma}(\xi,v) + \bm{\Gamma}(\xi, \xi) \\
  & = \bm{\Gamma}(v,v)-2\bm{\Gamma}(\xi,v)+\bm{\Gamma}(\xi,\xi).
  \end{aligned}
\]
Replacing $\bm{\Gamma}$ in eq.~\eqref{eq:vector-square} with this expansion, we
obtain
\[
  a -2\tilde{b}(v)+\bfC(v,v) = \bigl[\kappa+\bm{\Gamma}(\xi,\xi)\bigr] -2\bm{\Gamma}(\xi,v) + \bm{\Gamma}(v,v)
\]
from which we conclude: $\bm{\Gamma}(v,v) = \bfC(v,v)$ (equating terms
“quadratric in $v$”); $\bm{\Gamma}(\xi, v) = \tilde{b}(v)$ (equating terms
linear in $v$); and $\kappa+\bm{\Gamma}(\xi,\xi)=a$ (equating constant terms).

The first and third of these identifications are clear. We should
choose $\bm{\Gamma}=\bfC$ and therefore $\kappa=a-\bfC(\xi,\xi)$. The second term is
less obvious. Replacing $\bm{\Gamma}$ with $\bfC$, we find
$\bfC(\xi, v) = \tilde{b}(v)$. What meaning should we ascribe to this?
Recall the meaning of $\bfC(\xi,v)$: it is notation for
$(\bfC(\xi))(v)$, or “$\bfC$ applied first to $\xi$; the result then
applied to~$v$.
\begin{marginfigure}
  \begin{center}
    \asyinclude[width=5cm]{bilinear-form2.asy}
  \end{center}
\caption{A vector space $V$ and its dual $V^*$, showing an element $v\in
  V$ and its image in $V^*$ under $\bfC$, as well as an element
  $\tilde{b}\in V^*$ and its image in $V$ under~$\bfC^{-1}$.\label{fig:bilinear-form2}}
\end{marginfigure}
That is, $\bfC(\xi)$ is an element of~$V^*$, as is $\tilde{b}$. Moreover,
both of these give the same result when acting on any $v\in V$ and,
hence, are the same element of~$V^*$. That is, $\bfC(\xi) = \tilde{b}$. 

A candidate answer for the minimiser of $f(v)$, is therefore
\begin{equation}
  \label{eq:minimiser}
  \xi = \bfC^{-1}(\tilde{b}).
\end{equation}
Unfortunately, we are not yet done. To conclude that
$\bfC^{-1}(\tilde{b})$ is a minimiser we must in addition show two
things: first, that $\bfC$ \emph{has} an inverse; and second that
$f(v)$ is indeed a minimum at this value.

It is convenient to tackle the second property first. Assume, for the
moment, that $\bfC$ is invertible and that $\xi$ is given by
eq.~\eqref{eq:minimiser}. This $\xi$ will be a minimiser of $f$ if
$f(v)>f(\xi)$ for all $v \neq \xi$ which, from eq.~\eqref{eq:vector-square},
is equivalent to the condition $\bfC(v-\xi,v-\xi)>\bfC(0,0)$. Since
$v$ is arbitrary and since $\bfC(0,0)=0$ this condition is equivalent
to
\begin{equation}
  \label{eq:positive-definite}
  \bfC(x,x) > 0 \quad\text{for all $x\in V$ such that $x\neq0$}.
\end{equation}
A symmetric bilinear form for which eq.~\eqref{eq:positive-definite}
holds is said to be \emph{positive definite}. The $\xi$ given by
eq.~\eqref{eq:minimiser}, if it exists, will be a minimiser if $\bfC$
is positive definite. Now we return to the issue of whether $\bfC$ is
invertible. In fact, it turns out that positive-definiteness implies
invertibility.

\emph{Theorem}: Any positive-definite, bilinear form on a
finite-dimensional vector space is invertible.

\emph{Proof}: A positive-definite, bilinear form, $\bfC$, is
invertible if it is injective and surjective. It suffices to show
injectivity, since the dimension of $V$ is the same as the dimension
of~$V^*$. To show injectivity suppose, for contradiction, that there
is some $u\neq\bzero$ in $V$ such that $\bfC(u)=\bzero$ (with the
right-hand side being the zero element of~$V^*$, and noting that this
condition is equivalent to injectivity). Then we would have
$\bfC(u, x) =0$ for any $x$; whence, in particular, $\bfC(u,u)=0$,
contradicting the assumed positive-definiteness of~$\bfC$.

All these remarks lead to the following result: Let $V$ be a
finite-dimensional vector space and suppose that $a$ is a number,
$\tilde{b}$ an element of the dual of $V$, and $\bfC$ a
positive-definite, symmetric bilinear form on $V$. Then,
\[
  \argmin_{v\in V} \Bigl[a +\tilde{b}(v) + \bfC(v,v) \Bigr] = \bfC^{-1}(\tilde{b}). 
\]

Here is a variation on the problem. Let $\bm{T}:V\to V$ be a linear map,
not necessarily invertible, and $s\in V$ a fixed element
of~$V$. Minimise the same quadratic form as before, now subject to the
constraint $\bm{T}(v)= s$.

Observe that if $s=\bzero$ then the constraint says that $v$ lies in
the null space of $T$. For any other $s$, the constraint says that $v$
lies in a coset of the null space of~$T$. 







\end{document}
