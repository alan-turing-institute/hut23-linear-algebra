\documentclass[10pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{beton}
\usepackage{eulervm}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{microtype}
%\usepackage[medium, compact]{titlesec}
\usepackage[inline]{asymptote}
\DeclareFontSeriesDefault[rm]{bf}{sbc}
% \usepackage{amssymb}
%% Turing grid is 21 columns (of 1cm if we are using A4)
%% Usually 4 "big columns", each of 4 text cols plus 1 gutter col;
%% plus an additional gutter on the left.
\usepackage[left=1cm, textwidth=11cm, marginparsep=1cm, marginparwidth=7cm]{geometry}
\usepackage[Ragged, size=footnote, shape=up]{sidenotesplus}
%% We used to use a two-column layout
% \setlength{\columnsep}{1cm}
\title{Simple optimisation (on vector spaces)}
\author{James Geddes}
\date{\today}
%%
\DeclareBoldMathCommand{\setR}{R}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\eg}{\emph{Example:}}
\newcommand{\ie}{\emph{i.e.}}
\hyphenation{anti-sym-met-ric}
\begin{document}
\maketitle

A classic problem is that of finding the location of the minimum of
some real-valued function. It is a hard problem! It is not just that
the function itself might be complicated in some way; the
\emph{domain} of the function might complicated: it could be discrete,
or high-dimensional, or have a complicated shape. All of these can
prevent an analytic solution and impede a numerical one.

The general problem is this. Suppose $X$ is some set, possibly with
additional structure, and $f\colon \setR \to \setR$ a real-valued function
on~$X$. We are to find $x_\text{min}\in V$ (if one exists) such that
\[
 f(x_\text{min}) \leq f(x) \quad\text{for all $x\in V$}.  
\]
The value $x_\text{min}$ is called the \emph{minimiser}
of~$f$. Alternatively, we say
\begin{equation}
  x_\text{min} = \argmin_{x\in V} f(x).
\label{eq:argmin}
\end{equation}

Sometimes, however, the domain of the function is, or can be
approximated by, a vector space; and then the problem may be more
tractable. To get a sense for the problem in this case, suppose for
the moment that the domain of $f$ is the reals. Here are a few, simple
examples where the minimum can be found without much difficulty.

\eg{} A constant function, $f(x) = a$. Such a function attains its
minimum value everywhere. To say it another way, there is no unique
point at which it is a minimum.

\eg{} A linear function,\sidenote{There is a slight inconsistency in
  terminology here.  A linear \emph{map} from one vector space to
  another necessarily maps zero to zero, whereas a linear function,
  like this one, may have a constant term.} $f(x) = a + bx$. This
function has no minimum (assuming $b\neq0$). Its value can be made
arbitrarily negative by taking $x$ sufficiently large.

\eg{} A quadratic, $f(x) = a + bx + cx^2$. The previous examples could
be solved by inspection. One way to approach the general case is to
start with the observation that, at any minimum of a function, its
derivative must be zero and its second derivative must be strictly
positive.\sidenote{Other values of the second derivative indicate
  maxima ($f''(x) < 0$) or “points of inflection” ($f''(x)=0$).}
However, for the case of a quadratic, there is a well-known “trick,”
which, as a bonus, also demonstrates that a mimumum is a global
minimum. Suppose we can find numbers $\kappa$, $\gamma$, and $\xi$ such that
\begin{equation}
  a + bx + cx^2 = \kappa + \gamma{(x - \xi)}^2.
\label{eq:completing-the-square}
\end{equation}
Since adding a constant to a function does not change the
\emph{location} of any mimimum, we have only to find the minimiser of
$\gamma{(x-\xi)}^2$. And, since ${(x-\xi)}^2$ is always non-negative, and zero
only when $x-\xi$, there will be a minimum only when $\gamma >0$, and that
minimum will be at $x=\xi$.
\begin{marginfigure}
  \begin{center}
    \asyinclude[width=4cm, height=4cm, keepAspect=false]{quadr.asy}
  \end{center}
\caption{A graph of $f(x) = 9 - 8x + 2x^2$. The minimum occurs at
  $x=2$, which may be sees this by rewriting $f$ as $f(x) =
  2{(x-2)}^2+1$.\label{fig:quadratic}}
\end{marginfigure}
The values of $\kappa$, $\gamma$, and $\xi$ can be read off from the equation
above: equating terms in $x^2$ gives $\gamma=c$ and equating terms in
$x$ gives $\xi=-\frac{1}{2}c^{-1}b$. (The value of $\kappa$ does not affect
the location of the minimum but for completeness it is
$\kappa=a-\frac{1}{4}cb^2$.) This trick is known as “completing the
square.”

We return, now, to the multidimensional case. Fix, once and for all, a
finite-dimensional, real vector space, $V$, not necessarily
one-dimensional. What is the equivalent, on $V$, of a quadratic
function on~$\setR$? We would like to write something of the form,
“$f(v) = a+bv+cv^2$” but in order to do so we must attach a meaning to
each of the terms.

It is clear what is meant by the constant term, $a$. A constant
function on $V$ is just a constant, $f(v)=a$.\sidenote{Note, again,
  that a constant function $f(v) = a$ is \emph{not} a linear map
  $V\to\setR$, since $f(0) \neq 0$.}

To make sense of the linear term, we must find some way of forming a
number from a vector, while “respecting the vector space structure of
$V$.” An obvious candidate for such a map is an element of $\mathcal{L}(V,
\setR)$. This space, $\mathcal{L}(V, \setR)$, itself has the structure of a
vector space. It is known as the \emph{dual} of $V$ and is
denoted~$V^*$. A term that is “linear in $v$” is therefore an
expression of the form $\tilde{b}(v)$ for some $\tilde{b}\in V^*$.
\marginpar{The vector space structure on $V^*$ is as follows. For
  $\tilde{p}, \tilde{q}\in V^*$ and $\alpha\in\setR$, the linear combination
  $\tilde{p}+\alpha\cdot\tilde{q}$ is that element of $V^*$ whose action on any
  vector $x\in V$ is $(\tilde{p}+\alpha\cdot\tilde{q})(x) = \tilde{p}(x) +
  \alpha\tilde{q}(x)$. \sidepar

  Suppose $(e_1, \dotsc, e_n)$ is a basis for~$V$. To define an
  element of $V^*$ it suffices to give its action on a basis
  of~$V$. Define $\tilde{e}^j\in V^*$ by
  \[
      \tilde{e}^j(e_i) = 
      \begin{cases}
        1 & \text{if $i=j$}, \\
        0 & \text{otherwise.}
      \end{cases}
  \]
  These $\tilde{e}^j$ are a basis for~$V^*$, known as the dual
  basis. \sidepar

  \hrule
}

It is less clear what might be meant by a “quadratic term.” It would
seem to involve the “product of a vector with itself,” and that is not
an operation that is obviously available in a general vector
space. Once again, we make use of the dual space. The idea is to start
with $v$, somehow “carry it across” to $V^*$, and then act with the
result on~$w$.

Thus, let $C\colon V\to V^*$ be a linear map from $V$ to its dual. For
any vector $v\in V$, we obtain $C(v)\in V^*$, a linear map from $V$
to~$\setR$ (see figure~\ref{fig:bilinear-form}). Since an element of $V^*$ is a linear map from $V$
to~$\setR$, we may apply $C(v)$ to $w\in V$ and thereby obtain a number,
$(C(v))(w)$. 

\begin{marginfigure}
  \begin{center}
    \asyinclude[width=5cm]{bilinear-form.asy}
  \end{center}
  \caption{A vector space $V$ and its dual $V^*$, showing: an element
    $x\in V$; a linear map $C\colon V\to V^*$; and the image of $x$ in
    $V^*$ under $C$.\label{fig:bilinear-form}}
\end{marginfigure}
In a sense, one may think of $C$ as a map, from pairs
$(v,w)\in V\times V$ to the reals, which is “linear in both $v$ and
$w$.” This view suggests a less cumbersome notation: instead of
$(C(v))(w)$ we shall write $C(v,w)$. Thus, by $C(v, w)$ we shall mean,
“apply $C$ to $v$, obtaining an elemement of $V^*$, and apply this
element to $w$, obtaining a number.” When $C$ is viewed from this
perspective, it is known as a \emph{bilinear form}.

\eg{} For $C$ any bilinear form,
$C(\alpha v, \beta w) = \alpha\beta C(v,w)$ (which very much gives $C$ the flavour of a
product).

We can now say roughly what is meant by a “quadratic term:” it is an
expression of the form $C(v,v)$ for some bilinear form~$C$.

Notice, however, that in this expression $C$ is applied to a single
$v$ (twice); whereas more generally a bilinear form may be applied to
two different vectors. Is there some redundancy in this definition?
Let $A$ be any bilinear form such that $A(v,w)=-A(w,v)$ and consider
the bilinear form $C+A$. By linearity, we have $(C+A)(v, v) =
C(v,v)+A(v,v)$. However, $A(v,v)=-A(v,v)$ (by assumption), whence
$A(v,v)=0$. Thus $(C+A)(v,v)=C(v,v)$; that is, $C+A$ gives rise to the
same quadratic form as~$C$.

A bilinear form $A$ for which $A(v,w)=-A(w,v)$ is said to be
\emph{antisymmetric}. Conversely, a bilinear form $S$ for which
$S(v,w)=S(w,v)$ is said to be \emph{symmetric}. Let $C$ be any
bilinear form and consider the identity:
\[
  C(v, w) = \frac{1}{2}\bigl[C(v,w) + C(w,v)\bigr]
  + \frac{1}{2}\bigl[C(v,w) - C(w,v)\bigr].
\]
The first term on the right-hand side is symmetric whereas the second
is antisymmetic. Since the antisymmetric term vanishes when both
arguments are the same, we may, without loss of generality, assume
that $C$ is symmetric when evaluting~$C(v,v)$.\sidenote{Well, we have
  to show that every symmetric bilinear form arises in this way.}

We are now in a position to say what we mean by a quadratic function on
a vector space. It is a function of the form:
\begin{equation}
  f(v) = a - 2\tilde{b}(v) + C(v, v).
  \label{eq:quadratic-function}
\end{equation}
In this expression, $a$ is a number, $\tilde{b}$ is an element of the
dual space and $C$ is a symmetric bilinear form. (The factor of $-2$
is conventional as it simplifies certain calculations.)

Having written down a function on $V$, we return to the problem of
finding the location of its minimum.

By direct analogy with the one-dimensional case,
eq.~\eqref{eq:completing-the-square}, we might attempt to rewrite
eq.~\eqref{eq:quadratic-function} as:
\begin{equation}
  f(v) = \kappa + \Gamma(v - \xi, v - \xi),
  \label{eq:vector-square}
\end{equation}
where now $\kappa$ is a number, $\xi$ is a vector (which we hope will turn
out to be the minimiser of $f$!), and $\Gamma$ is a symmetric bilinear
form. (Note that, previously, the last term on the right-hand side
involved the expression ${(x-\xi)}^2$; here, a symmetric bilinear form
is required to effect the square.)

In the one-dimensional case we next expanded the term in ${(x-\xi)}^2$
and equated coefficients of each power of $x$. To do the same thing
here, we shall have to expand the term in $\Gamma$. Recall the meaning of
$\Gamma(v-\xi, v-\xi)$: $\Gamma$ is applied to $v-\xi$ to obtain an element
of~$V^*$; this element is then applied to $v-\xi$. Both of these
applications are linear, and so
\[
  \begin{aligned}
  \Gamma(v-\xi,v-\xi) & = \Gamma(v-\xi,v)-\Gamma(v-\xi, \xi) \\
  & = \Gamma(v,v)-\Gamma(v, \xi) - \Gamma(\xi,v) + \Gamma(\xi, \xi) \\
  & = \Gamma(v,v)-2\Gamma(\xi,v)+\Gamma(\xi,\xi).
  \end{aligned}
\]
Replacing $\Gamma$ in eq.~\eqref{eq:vector-square} with this expansion, we
obtain
\[
  a -2\tilde{b}(v)+C(v,v) = \bigl[\kappa+\Gamma(\xi,\xi)\bigr] -2\Gamma(\xi,v) + \Gamma(v,v)
\]
from which we conclude: $\Gamma(v,v) = C(v,v)$ (from the
terms “quadratric in $v$”); $\Gamma(\xi, v) = \tilde{b}(v)$ (from the terms
linear in $v$); and $\kappa+\Gamma(\xi,\xi)=a$ (from the constant terms).

The first and third of these identifications are clear. We should
choose $\Gamma=C$ and therefore $\kappa=a-C(\xi,\xi)$. The second term is less
obvious. Replacing $\Gamma$ with $C$, it is
$C(\xi, v) = \tilde{b}(v)$. What meaning should we ascribe to this?
Recall the meaning of $C(\xi,v)$: it is notation for $(C(\xi))(v)$, or
“$C$ applied first to $\xi$, and the result is applied to~$v$ (see
figure~\ref{fig:bilinear-form2}).
\begin{marginfigure}
  \begin{center}
    \asyinclude[width=5cm]{bilinear-form2.asy}
  \end{center}
\caption{A vector space $V$ and its dual $V^*$, showing an element $x\in
  V$ and its image in $V^*$ under $C$, as well as an element
  $\tilde{b}\in V^*$ and its image in $V$ under~$C^{-1}$.\label{fig:bilinear-form2}}
\end{marginfigure}
That is, $C(\xi)$ is an element of~$V^*$, as is $\tilde{b}$. Moreover,
both of these give the same result when acting on any $v\in V$ and,
hence, are the same element of~$V^*$. That is, $C(\xi) = \tilde{b}$. 

A candidate answer for the minimiser of $f(v)$, is therefore
\begin{equation}
  \label{eq:minimiser}
  \xi = C^{-1}(\tilde{b}).
\end{equation}
Unfortunately, we are not yet done. To conclude that this is a
minimiser we must in addition show two things: first, that $C$
\emph{has} an inverse; and second that $f(v)$ is indeed a minimum at
this value.

It is convenient to tackle the second condition first. Assume, for the
moment, that $C$ is invertible and that $\xi$ is given by
eq.~\eqref{eq:minimiser}. This $\xi$ will be a minimiser of We must show that $f(v)>f(\xi)$ for all
$v\neq x$ which, from eq.~\eqref{eq:vector-square}, is equivalent to
requiring $C(v-\xi,v-\xi)>C(0,0)$. Since $v$ is arbitrary and
$C(0,0)=0$ this condition is equivalent to
\begin{equation}
  \label{eq:positive-definite}
  C(x,x) > 0 \quad\text{for all $x\in V$ such that $x\neq0$}.
\end{equation}
A symmetric bilinear form for which eq.~\eqref{eq:positive-definite}
holds is said to be \emph{positive definite}. 





\end{document}
