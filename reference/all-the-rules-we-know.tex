%% -*- fill-column: 72; eval: (auto-fill-mode -1); eval: (visual-fill-column-mode 1); eval: (visual-line-mode 1); eval: (adaptive-wrap-prefix-mode 1) -*-
\documentclass[10pt, a4paper, twocolumn]{article}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage[margin=0.51in]{geometry}
\usepackage{parskip}
\usepackage{tabularx}
\usepackage{array}
\usepackage{changepage}
\usepackage[mathscr]{euscript}

\newcommand{\defn}[1]{\textbf{\textsf{#1}}}
\newcommand{\set}[1]{\mathbold{#1}}
\newcommand{\imag}{\mathrm{i}}
\newcommand{\card}[1]{\##1}
\newcommand{\transpose}[1]{{#1}^{\rm t}}
\newcommand{\inner}[2]{\left\langle #1, #2 \right\rangle}
\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}
\def\R{\mathbb{R}}
\def\C{\mathbb{C}}
\def\F{\mathbb{F}}
\def\setsep{\mid}
\def\vspan{\text{span}}
\def\enumfix{\mbox{ } \vspace*{-1.2\baselineskip}}
\def\L{\mathscr{L}}
\def\M{\mathscr{M}}
\DeclareMathOperator{\kernel}{null}
\DeclareMathOperator{\range}{range}
\DeclareMathOperator{\Real}{Re}
\DeclareMathOperator{\Imag}{Im}

\newtheoremstyle{break}% name
  {}%          Space above, empty = `usual value'
  {8pt}%       Space below
  {\upshape}%  Body font
  {}%          Indent amount (empty = no indent, \parindent = para indent)
  {\bfseries}% Thm head font
  {.}%         Punctuation after thm head
  {\newline}%  Space after thm head: \newline = linebreak
  {}%          Thm head spec

\theoremstyle{break}
\newtheorem{innerdefinition}{Definition}

\theoremstyle{break}
\newtheorem{innerproperty}{Property}

\theoremstyle{break}
\newtheorem{innernotation}{Notation}

\theoremstyle{break}
\newtheorem{innerresult}{Result}

\theoremstyle{break}
\newtheorem{innerlemma}{Lemma}

\theoremstyle{break}
\newtheorem{innercorollary}{Corollary}

\theoremstyle{break}
\newtheorem{innertheorem}{Theorem}

\newenvironment{definition}[1]
  {\renewcommand\theinnerdefinition{#1}\innerdefinition}
  {\endinnerdefinition}

\newenvironment{property}[1]
  {\renewcommand\theinnerproperty{#1}\innerproperty}
  {\endinnerproperty}

\newenvironment{notation}[1]
  {\renewcommand\theinnernotation{#1}\innernotation}
  {\endinnernotation}

\newenvironment{result}[1]
  {\renewcommand\theinnerresult{#1}\innerresult}
  {\endinnerresult}

\newenvironment{lemma}[1]
  {\renewcommand\theinnerlemma{#1}\innerlemma}
  {\endinnerlemma}

\newenvironment{corollary}[1]
  {\renewcommand\theinnercorollary{#1}\innercorollary}
  {\endinnercorollary}

\newenvironment{theorem}[1]
  {\renewcommand\theinnertheorem{#1}\innertheorem}
  {\endinnertheorem}

\title{All the rules we know}
\date{13 October 2023}
\author{}

\renewcommand{\arraystretch}{1.2}
\setlength{\tabcolsep}{3pt}

\newenvironment{forceindent}{\begin{adjustwidth}{16pt}{}}{\end{adjustwidth}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Chapter 1.A}

\begin{definition}{1.1}[Complex Numbers]
A \defn{complex number}\/ is an ordered pair $(a, b)$, where $a, b \in \R$, but we will write this as $a + bi$.

The set of all complex numbers is denoted by $\C$:
$$
\C = \{ a + bi \setsep a, b \in \R \}.
$$

\defn{Addition and multiplication} on $\C$ are defined by
\begin{align*}
(a + bi) + (c + di) &= (a + c) + (b + d)i, \\
(a + bi)(c + di) &= (ac - bd) + (ad + bc)i;
\end{align*}
here $a, b, c, d \in \R$.
\end{definition}

\begin{property}{1.3}[Properties of real arithmetic]
Intentionally restricted to $\R$ for the sake of the 1.A exercises. Equivalent properties can be derived for $\C$.

\defn{commutativity}
\begin{forceindent}
$\alpha + \beta = \beta + \alpha$ and $\alpha \beta = \beta \alpha$ for all $\alpha, \beta \in \R$;
\end{forceindent}

\defn{associativity}
\begin{forceindent}
$(\alpha + \beta) + \lambda = \alpha + (\beta + \lambda)$ and $(\alpha \beta) \lambda = \alpha (\beta \lambda)$ for all $\alpha, \beta, \lambda \in \R$;
\end{forceindent}

\defn{identities}
\begin{forceindent}
$\lambda + 0 = \lambda$ and $\lambda 1 = \lambda$ for all $\lambda \in \R$;
\end{forceindent}

\defn{additive inverse}
\begin{forceindent}
for every $\alpha \in \R$, there exists a unique $\beta \in \R$ such that $\alpha + \beta = 0$;
\end{forceindent}

\defn{multiplicative inverse}
\begin{forceindent}
for every $\alpha \in \R$ with $\alpha \not= 0$, there exists a unique $\beta \in \R$ such that $\alpha \beta = 1$;
\end{forceindent}

\defn{distributive property}
\begin{forceindent}
$\lambda (\alpha + \beta) = \lambda \alpha + \lambda \beta$ for all $\lambda, \alpha, \beta \in \R$.
\end{forceindent}
\end{property}

\begin{definition}{1.5}[ $-\alpha$, subtraction, $1 / \alpha$, division]
Let $\alpha, \beta \in \C$.

Let $-\alpha$ denote the additive inverse of $\alpha$. Thus $-\alpha$ is the unique complex number such that
$$
\alpha + (-\alpha) = 0.
$$

\defn{Subtraction} on $\C$ is defined by
$$
\beta - \alpha = \beta + (-\alpha).
$$

For $\alpha \not= 0$, let $1 / \alpha$ denote the multiplicative inverse of $\alpha$. Thus $1 / \alpha$ is the unique complex number such that
$$
\alpha (1 / \alpha) = 1.
$$

\defn{Division} on $\C$ is defined by
$$
\beta / \alpha = \beta (1 / \alpha).
$$
\end{definition}

\begin{notation}{1.6}[$\F$]
$\F$ stands for either $\R$ or $\C$.
\end{notation}

\begin{definition}{1.8}[list, length]
Suppose $n$ is a nonnegative integer. A \defn{list} of \defn{length} $n$ is an ordered collection of $n$ elements separated by commas and surrounded by parentheses. A list of length $n$ looks like this:
$$
(x_1, \ldots, x_n ).
$$

Two lists are equal if and only if they have the same length and the same elements in the same order.
\end{definition}

\begin{notation}{1.10}[notation: $n$]
$n$ represents a positive integer.
\end{notation}

\begin{definition}{1.11}[$\F^n$, coordinate]
$\F^n$ is the set of all lists of length $n$ of elements of $F$:
$$
\F^n = \{ (x_1, \ldots, x_n ) \setsep x_j \in \F \text{ for } j = 1, \ldots, n \}.
$$

For $(x_1, \ldots, x_n ) \in \F^n$ and $j \in \{ 1, \ldots, n \}$, we say that $x_j$ is the $j^{\text{th}}$ \defn{coordinate} of $(x_1, \ldots, x_n )$.
\end{definition}

\begin{definition}{1.13}[addition in $\F^n$]
\defn{Addition} in $\F^n$ is defined by adding corresponding coordinates:
$$
(x_1, \ldots, x_n ) + (y_1, \ldots, y_n ) = (x_1 + y_1, \ldots, x_n + y_n ).
$$
\end{definition}

\begin{definition}{1.15}[$0$]
Let $0$ denote the list of length $n$ whose coordinates are all 0:
$$
(0, \ldots, 0 ).
$$
\end{definition}

\begin{definition}{1.17}[additive inverse in $\F^n$]
For $x \in \F^n$, the \defn{additive inverse} of $x$, denoted $-x$, is the vector $-x \in \F^n$ such that
$$
x + (-x) = 0.
$$
In other words, if $x = (x_1, \ldots, x_n)$, then $-x = (-x_1, \ldots, -x_n)$.
\end{definition}

\begin{definition}{1.18}[scalar multiplication in $\F^n$]
The \defn{product} of a number $\lambda$ and a vector in $\F^n$ is computed by multiplying each coordinate of the vector by $\lambda$:
$$
\lambda (x_1, \ldots, x_n) = (\lambda x_1, \ldots, \lambda x_n);
$$
Here $\lambda \in \F$ and $(x_1, \ldots, x_n) \in \F^n$.
\end{definition}

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Chapter 1.B}

\begin{definition}{1.19}[addition, scalar multiplication]
An \defn{addition} on a set $V$ is a function that assigns an element $u + v \in V$ to each pair of elements $u, v \in V$.

A \defn{scalar multiplication} on a set $V$ is a function that assigns an element $\lambda v \in V$ to each $\lambda \in \F$ and each $v \in V$.
\end{definition}

\begin{definition}{1.20}[vector space]
A \defn{vector space over $\F$} is a set $V$ along with an addition on $V$ and a scalar multiplication on $V$ such that the following properties hold:

\defn{commutativity}
\begin{forceindent}
$u + v = v + u$ for all $u, v \in V$;
\end{forceindent}

\defn{associativity}
\begin{forceindent}
$(u + v) + w = u + (v + w)$ and $(ab)v = a(bv)$ for all $u, v, w \in V$ and all $a, b \in \F$;
\end{forceindent}

\defn{additive identity}
\begin{forceindent}
there exists an element $0 \in V$ such that $v + 0 = v$ for all $v \in V$;
\end{forceindent}

\defn{additive inverse}
\begin{forceindent}
for every $v \in V$, there exists $w \in V$ such that $v + w = 0$;
\end{forceindent}

\defn{multiplicative identity}
\begin{forceindent}
$1v = v$ for all $v \in V$;
\end{forceindent}

\defn{distributive property}
\begin{forceindent}
$a (u + v) = au + av$ and $(a + b)v = av + bv$ for all $a, b \in \F$ and all $u, v \in V$.
\end{forceindent}
\end{definition}

\begin{definition}{1.21}[vector, point]
Elements of a vector space are called \defn{vectors} or \defn{points}.
\end{definition}

\begin{definition}{1.22}[real vector space, complex vector space]
A vector space over $\R$ is called a \defn{real vector space}.

A vector space over $\C$ is called a \defn{complex vector space}.
\end{definition}

\begin{notation}{1.24}[$\F^S$]
If $S$ is a set, $\F^S$ denotes the set of functions from $S$ to $\F$.

For $f, g \in \F^S$ the \defn{sum} $f + g \in \F^S$ is the function defined by
$$
(f + g)(x) = f(x) + g(x)
$$
for all $x \in S$.

For $\lambda \in \F$ and $f \in \F^S$, the \defn{product} $\lambda f \in \F^S$ is the function defined by
$$
(\lambda f)(x) = \lambda f(x)
$$
for all $x \in S$.
\end{notation}

\begin{notation}{1.28}[$-v, w - v$]
Let $v, w \in V$. Then
\begin{enumerate}
\item $-v$ denotes the additive inverse of $v$;
\item $w - v$ is defined to be $w + (-v)$.
\end{enumerate}
\end{notation}

\begin{notation}{1.29}[$V$]
$V$ denotes a vector space over $\F$.
\end{notation}

\newpage

The following rules can all derived from the definition of a vector space.

\begin{result}{1.26}[unique additive identity]
A vector space has a unique additive identity.
\end{result}

\begin{result}{1.27}[unique additive inverse]
Every element in a vector space has a unique additive inverse.
\end{result}

\begin{result}{1.30}[the number $0$ times a vector]
$0v = 0$ for every $v \in V$.
\end{result}

\begin{result}{1.31}[a number times the vector $0$]
$a0 = 0$ for every $a \in \F$.
\end{result}

\begin{result}{1.32}[the number $-1$ times a vector]
$(-1)v = -v$ for every $v \in V$.
\end{result}

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Chapter 1.C}

\begin{definition}{1.33}[subspace]
A subset $U$ of $V$ is called a \defn{subspace} of $V$ if $U$ is also a vector space with the same additive identity, addition, and scalar multiplication as on $V$.
\end{definition}

\begin{definition}{1.36}[sum of subspaces]
Suppose $V_1, \ldots, V_m$ are subspaces of $V$. The \defn{sum} of $V_1, \ldots, V_m$, denoted by $V_1 + \cdots + V_m$, is the set of all possible sums of elements of $V_1, \ldots, V_m$. More precisely:
$$
V_1 + \cdots + V_m = \{v_1 + \cdots + v_m \setsep v_1 \in V_1, \ldots, v_m \in V_m \}.
$$
\end{definition}

\begin{definition}{1.41}[direct sum, $\oplus$]
Suppose $V_1, \ldots, V_m$ are subspaces of $V$.

\begin{enumerate}
\item The sum $V_1 + \cdots + V_m$ is called a \defn{direct sum} if each element of $V_1 + \cdots + V_m$ can be written in only one way as a sum $v_1 + \cdots + v_m$, where each $v_k \in V_k$.
\item If $V_1 + \cdots + V_m$ is a direct sum, then $V_1 \oplus \cdots \oplus V_m$ denotes $V_1 + \cdots + V_m$, with the $\oplus$ notation serving as an indication that this is a direct sum.
\end{enumerate}
\end{definition}

\newpage

The following rules can all be derived from the definitions.

\begin{result}{1.34}[conditions for a subspace]
A subset $U$ of $V$ is a subspace of $V$ if and only if $U$ satisfies the following three conditions.

\defn{additive identity}
\begin{forceindent}
$0 \in U$.
\end{forceindent}

\defn{closed under addition}
\begin{forceindent}
$u, w \in U$ implies $u + w \in U$.
\end{forceindent}

\defn{closed under scalar multiplication}
\begin{forceindent}
$a \in \F$ and $u \in U$ implies $au \in U$.
\end{forceindent}
\end{result}

\begin{result}{1.40}[sum of subspaces is the smallest containing subspace]
Suppose $V_1, \ldots, V_m$ are subspaces of $V$. Then $V_1 + \cdots + V_m$ is the smallest subspace of $V$ containing $V_1, \ldots, V_m$.
\end{result}

\begin{result}{1.45}[condition for a direct sum]
Suppose $V_1, \ldots, V_m$ are subspaces of $V$. Then $V_1 + \cdots + V_m$ is a direct sum if and only if the only way to write $0$ as a sum $v_1 + \cdots + v_m$, where each $v_k \in V_k$, is by taking each $v_k$ equal to $0$.
\end{result}

\begin{result}{1.46}[direct sum of two subspaces]
Suppose $U$ and $W$ are subspaces of $V$. Then
$$
U + W \text{ is a direct sum} \Longleftrightarrow U \cap W = \{ 0 \}.
$$
\end{result}

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Chapter 2.A}

\begin{notation}{2.1}[list of vectors]
We write lists of vectors without surrounding parentheses.
\end{notation}

\begin{definition}{2.2}[linear combination]
A \defn{linear combination} of a list $v_1, \ldots, v_m$ of vectors in $V$ is a vector of the form
$$
a_1 v_1 + \cdots + a_m v_m,
$$
where $a_1, \ldots, a_m \in \F$.
\end{definition}

\begin{definition}{2.4}[span]
The set of all linear combinations of a list of vectors $v_1, \ldots, v_m$ in $V$ is called the \defn{span} of $v_1, \ldots, v_m$, denoted by $\vspan(v_1, \ldots, v_m)$. In other words
$$
\vspan(v_1, \ldots, v_m) = \{ a_1 v_1 + \cdots + a_m v_m \setsep a_1, \ldots, a_m \in \F \}.
$$

The span of the empty list $(\ )$ is defined to be $\{ 0 \}$.
\end{definition}

\begin{definition}{2.7}[spans]
If $\vspan(v_1, \ldots, v_m)$ equals $V$, we say that the list $v_1, \ldots, v_m$ \defn{spans} $V$.
\end{definition}

\begin{definition}{2.9}[finite-dimensional vector space]
A vector space is called \defn{finite-dimensional} if some list of vectors in it spans the space.
\end{definition}

\begin{definition}{2.10}[polynomial, $\mathscr{P}(\F)$] \enumfix
\begin{enumerate}
\item A function $p : \F \to \F$ is called a \defn{polynomial} with coefficients in $\F$ if there exist $a_0, \ldots, a_m \in \F$ such that
$$
p(z) = a_0 + a_1 z + a_2 z^2 + \cdots + a_m z^m
$$
for all $z \in \F$.
\item $\mathscr{P}(\F)$ is the set of all polynomials with coefficients in $\F$.
\end{enumerate}
\end{definition}

\begin{definition}{2.11}[degree of a polynomial, $\deg\ p$] \enumfix
\begin{enumerate}
\item A polynomial $p \in \mathscr{P}(\F)$ is said to have \defn{degree $m$} if there exist scalars $a_0, a_1, \ldots, a_m \in \F$ with $a_m \not= 0$ such that for every $z \in \F$, we have
$$
p(z) = a_0 + a_1 z + \cdots + a_m z^m.
$$
\item The polynomial that is identically $0$ is said to have degree $-\infty$.
\item The degree of polynomial $p$ is denoted by $\deg\ p$.
\end{enumerate}
\end{definition}

\begin{notation}{2.12}[$\mathscr{P}_m (\F)$]
For $m$ a nonnegative integer, $\mathscr{P}_m (\F)$ denotes the set of all polynomials with coefficients in $\F$ and degree at most $m$.
\end{notation}

\begin{definition}{2.13}[infinite-dimensional vector space]
A vector space is called \defn{infinite-dimensional} if it is not finite-dimensional.
\end{definition}

\newpage

\begin{definition}{2.15}[linearly independent] \enumfix
\begin{enumerate}
\item A list $v_1, \ldots, v_m$ of vectors in $V$ is called \defn{linearly independent} if the only choice of $a_1, \ldots, a_m \in \F$ that makes
$$
a_v v_1 + \cdots + a_m v_m = 0
$$
is $a_1 = \cdots = a_m = 0$.
\item The empty list $(\ )$ is also declared to be linearly independent.
\end{enumerate}
\end{definition}

\begin{definition}{2.17}[linearly dependent] \enumfix
\begin{enumerate}
\item A list of vectors $V$ is called \defn{linearly dependent} if it is not linearly independent.
\item In other words, a list $v_1, \ldots, v_m$ of vectors in $V$ is linearly dependent if there exist $a_1, \ldots, a_m \in \F$, not all $0$, such that $a_1 v_1 + \cdots + a_m v_m = 0$.
\end{enumerate}
\end{definition}

\vspace{17\baselineskip}

The following can be derived from the definitions.

\begin{result}{2.6}[span is the smallest containing subspace]
The span of a list of vectors in $V$ is the smallest subspace of $V$ containing all vectors in the list.
\end{result}

\begin{lemma}{2.19}[linear dependence lemma]
Suppose $v_1, \ldots, v_m$ is a linearly dependent list in $V$. Then there exists $k \in \{ 1, 2, \ldots, m \}$ such that
$$
v_k \in \vspan(v_1, \ldots, v_{k - 1}).
$$
Furthermore, if $k$ satisfies the condition above and the $k$\textsuperscript{th} term is removed from $v_1, \ldots, v_m$, then the span of the remaining list equals $\vspan(v_1, \ldots, v_m)$.
\end{lemma}

\begin{result}{2.22}[length of linearly independent list $\le$ length of spanning list]
In a finite-dimensional vector space, the length of every linearly independent list of vectors is less than or equal to the length of every spanning list of vectors.
\end{result}

\begin{result}{2.25}[finite-dimensional subspaces]
Every subspace of a finite-dimensional vector space is finite-dimensional.
\end{result}

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Chapter 2.B}

\begin{definition}{2.26}[basis]
A \defn{basis} of $V$ is a list of vectors in $V$ that is linearly dependent and spans $V$.
\end{definition}

\vspace{5\baselineskip}

\begin{result}{2.28}[criterion for basis]
A list $v_1, \ldots, v_n$ of vectors of $V$ is a basis of $V$ if and only if every $v \in V$ can be written uniquely in the form
$$
v = a_1 v_1 + \cdots + a_n v_n,
$$
where $a_1, \ldots, a_n \in \F$.
\end{result}

\begin{result}{2.30}[every spanning list contains a basis]
Every spanning list in a vector space can be reduced to a basis of the vector space.
\end{result}

\begin{corollary}{2.31}[basis of finite-dimensional vector space]
Every finite-dimensional vector space has a basis.
\end{corollary}

\begin{result}{2.32}[every linearly independent list extends to a basis]
Every linearly independent list of vectors in a finite-dimensional vector space can be extended to a basis of the vector space.
\end{result}

\begin{corollary}{2.33}[every subspace of $V$ is part of a direct sum equal to $V$]
Suppose $V$ is finite-dimensional and $U$ is a subspace of $V$. Then there is a subspace $W$ of $V$ such that $V = U \oplus W$.
\end{corollary}

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Chapter 2.C}

\begin{definition}{2.35}[dimension, $\dim V$]
The \defn{dimension} of a finite-dimensional vector space is the length of any basis of the vector space.

The dimension of a finite-dimensional vector space $V$ is denoted by $\dim V$.
\end{definition}

\vspace{2\baselineskip}

\begin{result}{2.34}[basis length does not depend on basis]
Any two bases of a finite-dimensional vector space have the same length.
\end{result}

\begin{result}{2.37}[dimension of a subspace]
If $V$ is finite-dimensional and $U$ is a subspace of $V$, then $\dim U \le \dim V$.
\end{result}

\begin{result}{2.38}[linearly independent list of the right length is a basis]
Suppose $V$ is finite-dimensional. Then every linearly independent list of vectors in $V$ of length $\dim V$ is a basis of $V$.
\end{result}

\begin{corollary}{2.39}[subspace of full dimension equals the whole space]
Suppose that $V$ is finite-dimensional and $U$ is a subspace of $V$ such that $\dim U = \dim V$. Then $U = V$.
\end{corollary}

\begin{result}{2.42}[spanning list of the right length is a basis]
Suppose $V$ is finite-dimensional. Then every spanning list of vectors in $V$ of length $\dim V$ is a basis of $V$.
\end{result}

\begin{result}{2.43}[dimension of sum]
If $V_1$ and $V_2$ are subspaces of a finite-dimensional vector space, then
$$
\dim(V_1 + V_2) = \dim V_1 + \dim V_2 - \dim(V_1 \cap V_2).
$$
\end{result}

\begin{result}{Page 48}[Sets and vector spaces]
\begin{tabular}{| *{2}{>{\raggedright}p{0.5\columnwidth} |}} \hline
{\bf sets} & {\bf vector spaces} \tabularnewline \hline
$S$ is a finite set & $V$ is a finite-dimensional vector space \tabularnewline \hline
$\card{S}$ & $\dim V$ \tabularnewline \hline
for subsets $S_1$, $S_2$ of $S$, the union $S_1 \cup S_2$ is the smallest subset of $S$ containing $S_1$ and $S_2$ & for subspaces $V_1$, $V_2$ of $V$, the sum $V_1 + V_2$ is the smallest subspace of $V$ containing $V_1$ and $V_2$ \tabularnewline \hline
$\card{(S_1 \cup S_2)} = \card{S_1} + \card{S_2} - \card{(S_1 \cap S_2)}$ & $\dim{(V_1 + V_2)} = \dim{V_1} + \dim{V_2} - \dim{(V_1 \cap V_2)}$ \tabularnewline \hline
$\card{(S_1 \cup S_2)} = \card{S_1} + \card{S_2} \Longleftrightarrow \card{(S_1 \cap S_2)} = \empty$ & $\dim{(V_1 + V_2)} = \dim{V_1} + \dim{V_2} \Longleftrightarrow \dim{(V_1 \cap V_2)} = \empty$ \tabularnewline \hline
$S_1 \cup \cdots \cup S_m$ is a disjoint union $\Longleftrightarrow \card{(S_1 \cup \cdots \cup S_M)} = \card{S_1} + \cdots + \card{S_M}$ & $V_1 \cdots + V_m$ is a direct sum $\Longleftrightarrow \dim{(V_1 + \cdots + V_m)} = \dim{V_1} + \cdots + \dim{V_m}$ \tabularnewline \hline
\end{tabular}
\end{result}

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Chapter 3.A}

\begin{definition}{3.1}[linear map]
A \defn{linear map} from $V$ to $W$ is a function $T : V \to W$ with the following properties.

\defn{additivity}
\begin{forceindent}
$T(u + v) = Tu + Tv$ for all $u, v \in V$.
\end{forceindent}

\defn{homogeneity}
\begin{forceindent}
$T(\lambda v) = \lambda(Tv)$ for all $\lambda \in \F$ and all $v \in V$.
\end{forceindent}

\end{definition}

\begin{notation}{3.2}[$\L(V, W), \L(V)$] \enumfix
\begin{enumerate}
\item The set of linear maps from $V$ to $W$ is denoted by $\L(V, W)$.
\item The set of linear maps from $V$ to $V$ is denoted by $\L(V)$. In other words, $\L(V) = \L(V, V)$.
\end{enumerate}
\end{notation}

\begin{definition}{3.5}[addition and scalar multiplication on $\L(V, W)$]
\label{def:addmultL}
Suppose $S, T \in \L(V, w)$ and $\lambda \in \F$. The \defn{sum} $S + T$ and the \defn{product} $\lambda T$ are the linear maps from $V$ to $W$ defined by
$$
(S + T)(v) = Sv + Tv \quad \text{ and } \quad (\lambda T)(v) = \lambda(Tv)
$$
for all $v \in V$.
\end{definition}

\begin{definition}{3.7}[product of linear maps]
\label{def:mapproduct}
If $T \in \L(U, V)$ and $S \in \L(V, W)$, then the \defn{product} $ST \in \L(U, V)$ is defined by
$$
(S T)(u) = S(Tu)
$$
for all $u \in U$.
\end{definition}

\newpage

\begin{lemma}{3.4}[linear map lemma]
Suppose $v_1, \ldots, v_n$ is a basis of $V$ and $w_1, \ldots, w_m \in W$. Then there exists a unique linear map $T : V \to W$ such that
$$
Tv_k = w_k
$$
for each $k = 1, \ldots, n$.
\end{lemma}

\begin{result}{3.6}[$\L(V, W)$ is a vector space]
With the operations of addition and scalar multiplication as in Definition \ref{def:addmultL}, $\L(V, W)$ is a vector space.
\end{result}

\begin{result}{3.8}[algebraic properties of products of linear maps]

\defn{associativity}
\begin{forceindent}
$(T_1 T_2) T_3 = T_1(T_2 T_3)$ whenever $T_1, T_2$ and $T_3$ are linear maps such that the products make sense (meaning $T_3$ maps into the domain of $T_2$ and $T_2$ maps into the domain of $T_1$).
\end{forceindent}

\defn{identity}
\begin{forceindent}
$T I = I T = T$ whenever $T \in \L(V, W)$; here the first $I$ is the identity operator on $V$ and the second $I$ is the identity operator on $W$.
\end{forceindent}

\defn{distributive properties}
\begin{forceindent}
$(S_1 + S_2)T = S_1 T + S_2 T$ and $S(T_1 + T_2) = S T_1 + S T_2$ whenever $T, T_1, T_2 \in \L(U, V)$ and $S, S_1, S_2 \in \L(V, W)$.
\end{forceindent}
\end{result}

\begin{result}{3.10}[linear maps take 0 to 0]
Suppose $T$ is a linear map from $V$ to $W$. Then $T(0) = 0$.
\end{result}

\begin{result}{Ex. 3A, 13}[Linear maps on a subspace can be extended to a map on the whole vector space]
Suppose $V$ is finite-dimensional. Prove that every linear map on a subspace of $V$ can be extended to a linear map on $V$. In other words, show that if $U$ is a subspace of $V$ and $S \in \L(U, V)$, then there exists $T \in \L(V, W)$ such that $Tu = Su$ for all $u \in E$.
\end{result}

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Chapter 3.B}

\begin{definition}{3.11}[null space, $\kernel T$]
For $T \in \L(V, W)$, the \defn{null space} of $T$, denoted by null $T$, is the subset of $V$ consisting of those vectors that $T$ maps to 0:
$$
\kernel T = \{ v \in V \setsep Tv = 0 \}.
$$
\end{definition}

\begin{definition}{3.14}[injective]
A function $T : V \to W$ is called \defn{injective} if $Tu = Tv$ implies $u = v$.
\end{definition}

\begin{definition}{3.16}[range]
For $T \in \L(V, W)$, the \defn{range} of $T$ is the subset $W$ consisting of those vectors that are equal to $Tv$ for some $v \in V$:
$$
\range T = \{ Tv \setsep v \in V \}.
$$
\end{definition}

\begin{definition}{3.19}[surjective]
A function $T : V \to W$ is called \defn{surjective} if its range equals $W$.
\end{definition}

\newpage

\begin{result}{3.13}[the null space is a subspace]
Suppose $T \in \L(V, W)$. Then $\kernel T$ is a subspace of $V$.
\end{result}

\begin{result}{3.15}[$\text{injectivity } \Leftrightarrow \text{ null space equals } \{ 0 \}$]
Let $T \in \L(V, W)$. Then $T$ is injective if and only if $\kernel T = \{ 0 \}$.
\end{result}

\begin{result}{3.18}[the range is a subspace]
If $T \in \L(V, W)$, then the range $T$ is a subspace of $W$.
\end{result}

\begin{theorem}{3.21}[fundamental theorem of linear maps]
Suppose $V$ is finite-dimensional and $T \in \L(V, W)$. Then $\range T$ is finite-dimensional and
$$
\dim V = \dim \kernel T + \dim \range T.
$$
\end{theorem}

\begin{result}{3.22}[linear map to a lower-dimensional space is not injective]
Suppose $V$ and $W$ are finite-dimensional vector spaces such that $\dim V > \dim W$. Then no linear map from $V$ to $W$ is injective.
\end{result}

\begin{result}{3.24}[linear map to a higher-dimensional space is not surjective]
Suppose $V$ and $W$ are finite-dimensional vector spaces such that $\dim V < \dim W$. Then no linear map from $V$ to $W$ is surjective.
\end{result}

\begin{result}{3.26}[homogeneous system of linear equations]
A homogeneous system of linear equations with more variables than equations has nonzero solutions.
\end{result}

\begin{result}{3.28}[inhomogeneous systems of linear equations]
An inhomogeneous system of linear equations with more equations than variables has no solution for some choice of the constant terms.
\end{result}

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Chapter 3.C}

\begin{definition}{3.29}[matrix, $A_{j, k}$]
Suppose $m$ and $n$ are nonnegative integers. An $m$-by-$n$ \defn{matrix} $A$ is a rectangular array of elements of $\F$ with $m$ rows and $n$ columns:
$$
A =
\begin{pmatrix}
A_{1, 1} & \cdots & A_{1, n} \\
\vdots & & \vdots \\
A_{m, 1} & \cdots & A_{m, n} \\
\end{pmatrix}
.
$$
The notation $A_{j, k}$ denotes the entry in row $j$, column $k$ of $A$.
\end{definition}

\begin{definition}{3.31}[matrix of a linear map, $\M(T)$]
Suppose $T \in \L(V, W)$ and $v_1, \ldots, v_n$ is a basis of $V$ and $w_1, \ldots, w_m$ is a basis of $W$. The \defn{matrix of $T$} with respect to these bases is the $m$-by-$n$ matrix $\M(T)$ whose entries $A_{j, k}$ are defined by
$$
T v_k = A_{1, k} w_1 + \cdots + A_{m, k} w_m .
$$
If the basis $v_1, \ldots, v_n$ and $w_1, \ldots, w_m$ are not clear from the context, then the notation $\M(T, (v_1, \ldots, v_n), (w_1, \ldots, w_m))$ is used.
\end{definition}

\begin{definition}{3.34}[matrix addition]
The \defn{sum of two matrices of the same size} is the matrix obtained by adding corresponding entries in the matrices:
\begin{align*}
& \begin{pmatrix}
A_{1, 1} & \cdots & A_{1, n} \\
\vdots & & \vdots \\
A_{m, 1} & \cdots & A_{m, n} \\
\end{pmatrix}
+
\begin{pmatrix}
C_{1, 1} & \cdots & C_{1, n} \\
\vdots & & \vdots \\
C_{m, 1} & \cdots & C_{m, n} \\
\end{pmatrix} \\
& \qquad \qquad \qquad \qquad =
\begin{pmatrix}
A_{1, 1} + C_{1, 1} & \cdots & A_{1, n} + C_{1, n} \\
\vdots & & \vdots \\
A_{m, 1} + C_{m, 1} & \cdots & A_{m, n} + C_{m, n} \\
\end{pmatrix}
.
\end{align*}
\end{definition}

\begin{definition}{3.36}[scalar multiplication of a matrix]
The product of a scalar and a matrix is the matrix obtained by multiplying each entry in the matrix by the scalar:
$$
\lambda
\begin{pmatrix}
A_{1, 1} & \cdots & A_{1, n} \\
\vdots & & \vdots \\
A_{m, 1} & \cdots & A_{m, n} \\
\end{pmatrix}
=
\begin{pmatrix}
\lambda A_{1, 1} & \cdots & \lambda A_{1, n} \\
\vdots & & \vdots \\
\lambda A_{m, 1} & \cdots & \lambda A_{m, n} \\
\end{pmatrix}
.
$$
\end{definition}

\begin{notation}{3.39}[$\F^{m, n}$]
For $m$ and $n$ positive integers, the set of all $m$-by-$n$ matrices with entries in $\F$ is denoted by $\F^{m, n}$.
\end{notation}

\begin{definition}{3.41}[matrix multiplication]
Suppose $A$ is an $m$-by-$n$ matrix and $B$ is an $n$-by-$p$ matrix. Then $A B$ is defined to be the $m$-by-$p$ matrix whose entry in row $j$, column $k$, is given by the equation
$$
(A B)_{j, k} = \sum_{r = 1}^n A_{j, r} B_{r, k} .
$$
Thus the entry in row $j$, column $k$, of $A B$ is computed by taking row $i$ of $A$ and column $k$ of $B$, multiplying together corresponding entries, and then summing.
\end{definition}

\begin{notation}{3.44}[$A_{j, \cdot}, A_{\cdot, k}$]
Suppose $A$ is an $m$-by-$n$ matrix.
\begin{enumerate}
\item If $1 \le j \le m$, then $A_{j, \cdot}$ denotes the 1-by-$n$ matrix consisting of row $j$ of $A$.
\item If $1 \le k \le n$, then $A_{\cdot, k}$ denotes the $m$-by-1 matrix consisting of column $k$ of $A$.
\end{enumerate}
\end{notation}

\begin{definition}{3.52}[column rank, row rank]
Suppose $A$ is an $m$-by-$n$ matrix with entries in $\F$.
\begin{enumerate}
\item The \defn{column rank} of $A$ is the dimension of the span of the columns of $A$ in $\F^{m, 1}$.
\item The \defn{row rank} of $A$ is the dimension of the span of the rows of $A$ in $\F^{1, n}$.
\end{enumerate}
\end{definition}

\begin{definition}{3.54}[transpose, $\transpose{A}$]
The \defn{transpose} of a matrix $A$, denoted by $\transpose{A}$, is the matrix obtained from $A$ by interchanging rows and columns. Specifically, if $A$ is an $m$-by-$n$ matrix, then $\transpose{A}$ is an $n$-by-$m$ matrix whose entries are given by the equation
$$
(\transpose{A})_{k, j} = A_{j, k} .
$$
\end{definition}

\begin{definition}{3.58}[rank]
The \defn{rank} of a matrix $A \in \F^{m, n}$ is the column rank of $A$.
\end{definition}

\newpage

\begin{result}{3.35}[matrix of the sum of linear maps]
Suppose $S, T \in \L(V, W)$. Then $\M(S + T) = \M(S) + \M(T)$.
\end{result}

\begin{result}{3.38}[the matrix of a scalar times a linear map]
Suppose $\lambda \in \F$ and $T \in \L(V, W)$. Then $\M(\lambda T) = \lambda \M(T)$.
\end{result}

\begin{result}{3.40}[$\dim \F^{m, n} = m n$]
Suppose $m$ and $n$ are positive integers. With addition and scalar multiplication defined as above, $\F^{m, n}$ is a vector space of dimension $m n$.
\end{result}

\begin{result}{3.43}[matrix of product of linear maps]
If $T \in \L(U, V)$ and $S \in \L(V, W)$, then $\M(S T) = \M(S) \M(T)$.
\end{result}

\begin{result}{3.46}[entry of matrix product equals row times column]
Suppose $A$ is an $m$-by-$n$ matrix and $B$ is an $n$-by-$p$ matrix. Then
$$
(A B)_{j, k} = A_{j, \cdot} B_{\cdot, k}
$$
if $1 \le j \le m$ and $1 \le k \le p$. In other words, the entry in row $j$, column $k$, of $A B$ equals (row $j$ of $A$) times (column $k$ of $B$).
\end{result}

\begin{result}{3.48}[column of matrix product equals matrix times column]
\label{result:column-product}
Suppose $A$ is an $m$-by-$n$ matrix and $B$ is an $n$-by-$p$ matrix. Then
$$
(A B)_{\cdot, k} = A B_{\cdot, k}
$$
if $1 \le k \le p$. In other words, column $k$ of $A B$ equals $A$ times column $k$ of $B$.
\end{result}

% Exercise 8 (row version of 3.48)
\begin{result}{Ex. 3C, 8}[row of matrix product equals matrix times row]
Suppose $A$ is an $m$-by-$n$ matrix and $B$ is an $n$-by-$p$ matrix. Then
$$
(A B)_{j, \cdot} = A_{j, \cdot} B
$$
if $1 \le j \le m$. In other words, row $j$ of $A B$ equals row $j$ of $A$ times $B$.

This is the row version of Result \ref{result:column-product}.
\end{result}

\begin{result}{3.50}[linear combination of columns]
\label{result:linear-combination-cols}
Suppose $A$ is an $m$-by-$n$ matrix and $b =
\left(\begin{smallmatrix}
b_1 \\
\vdots \\
b_n
\end{smallmatrix}\right)$
is an $n$-by-1 matrix. Then
$$
A b = b_1 A_{\cdot, 1} + \cdots + b_n A_{\cdot, n} .
$$
In other words, $A b$ is a linear combination of the columns of $A$, with the scalars that multiply the columns coming from $b$.
\end{result}

% Exercise 9 (row version of 3.50)
\begin{result}{Ex. 3C, 9}[linear combination of rows]
Suppose $a =
\left(\begin{smallmatrix}
a_1 & \ldots & a_n
\end{smallmatrix}\right)$
is a 1-by-$n$ matrix and $B$ is an $n$-by-$p$ matrix. Then
$$
a B = a_1 B_{1, \cdot} + \cdots + a_n B_{n, \cdot} .
$$
In other words, $a B$ is a linear combination of the rows of $B$, with the scalars that multiply the rows coming from $a$.

This is the row version of Result \ref{result:linear-combination-cols}.
\end{result}

\begin{result}{3.51}[matrix multiplication as linear combinations of columns]
Suppose $C$ is an $m$-by-$c$ matrix and $R$ is a $c$-by-$n$ matrix.
\begin{enumerate}
\item[(a)] If $k \in \{ 1, \ldots, n \}$, then column $k$ of $C R$ is a linear combination of the columns of $C$, with the coefficients of this linear combination coming from columns $k$ of $R$.
\item[(b)] If $j \in \{ 1, \ldots, m \}$, then row $j$ of $C R$ is a linear combination of the rows of $R$, with the coefficients of this linear combination coming from row $j$ of $C$.
\end{enumerate}
\end{result}

% Exercise 14
\begin{result}{Ex. 3C, 14}[transpose is a linear map]
Suppose $m$ and $n$ are positive integers. Then the function $A \mapsto \transpose{A}$ is a linear map from $\F^{m, n}$ to $\F^{n, m}$.

In other words $\transpose{(A + B)} = \transpose{A} + \transpose{B}$, $\transpose{(\lambda A)} = \lambda \transpose{A}$ for all $m$-by-$n$ matrices $A$, $B$ and all $\lambda \in \F$.
\end{result}

% Exercise 15
\begin{result}{Ex. 3C, 15}[The transpose of the product is the product of the transposes in the opposite order]
If $A$ is an $m$-by-$n$ matrix and $C$ is an $n$-by-$p$ matrix, then
$$
\transpose{(AC)} = \transpose{C} \transpose{A} .
$$
\end{result}

\begin{result}{3.56}[column-row factorisation]
Suppose $A$ is an $m$-by-$n$ matrix with entries in $\F$ and column rank $c \ge 1$. Then there exist an $m$-by-$c$ matrix $C$ and a $c$-by-$n$ matrix $R$, both with entries in $\F$, such that $A = C R$.
\end{result}

\begin{result}{3.57}[column rank equals row rank]
Suppose $A \in \F^{m, n}$. Then the column rank of $A$ equals the row rank of $A$.
\end{result}

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Chapter 3.D}

\begin{definition}{3.59}[invertible, inverse] \enumfix
\begin{enumerate}
\item A linear map $T \in \L(V, W)$ is called \defn{invertible} if there exists a linear map $S \in \L(W, V)$ such that $ST$ equals the identity operator on $V$ and $TS$ equals the identity operator on $W$.
\item A linear map $S \in \L(W, V)$ satisfying $ST = I$ and $TS = I$ is called an \defn{inverse} of $T$ (note that the first $I$ is the identity operator on $V$ and the second $I$ is the identity operator on $W$.
\end{enumerate}
\end{definition}

\begin{notation}{3.61}[$T^{-1}$]
If $T$ is invertible, then its inverse is denoted by $T^{-1}$. In other words, if $T \in \L(V, W)$ is invertible, then $T^{-1}$ is the unique element of $\L(W, V)$ such that $T^{-1} T = I$ and $T T^{-1} = I$.
\end{notation}

\begin{definition}{3.69}[isomorphism, isomorphic] \enumfix
\begin{enumerate}
\item An \defn{isomorphism} is an invertible linear map.
\item Two vector spaces are called \defn{isomorphic} if there is an isomorphism from one vector space onto the other one.
\end{enumerate}
\end{definition}

\begin{definition}{3.73}[matrix of a vector, $\M(v)$]
Suppose $v \in V$ and $v_1, \ldots, v_n$ is a basis of $V$. The \defn{matrix of $V$} with respect to this basis is the $n$-by-$1$ matrix
$$
\M(v) =
\begin{pmatrix}
b_1 \\
\vdots \\
b_n
\end{pmatrix},
$$
where $b_1, \ldots, b_n$ are the scalars such that
$$
v = b_1 v_1 + \cdots + b_n v_n .
$$
\end{definition}

\begin{definition}{3.79}[identity matrix, $I$]
Suppose $n$ is a positive integer. The $n$-by-$n$ matrix
$$
\begin{pmatrix}
1 & & 0 \\
 & \ddots & \\
0 & & 1
\end{pmatrix}
$$
with 1's on the diagonal (the entries where the row number equals the column number) and 0's elsewhere is called the \defn{identity matrix} and is denoted by $I$.
\end{definition}

\begin{definition}{3.80}[invertible, inverse, $A^{-1}$]
A square matrix $A$ is called \defn{invertible} if there is a square matrix $B$ of the same size such that $AB = BA = I$; we call $B$ the \defn{inverse} of $A$ and denote it by $A^{-1}$.
\end{definition}

% Exercise 9
\begin{notation}{Ex. 3D, 9}[Restriction of a map to a subset]
If $T : V \to W$ and $U \subseteq V$ then the \defn{restriction} $T|_U$ of $T$ to $U$ is the function $T : U \to W$ whose domain is $U$, with $T|_U$ defined by
$$
T|_U (u) = T(u)
$$
for every $u \in U$.
\end{notation}

\newpage

\begin{result}{3.60}[inverse is unique]
An invertible linear map has a unique inverse.
\end{result}

\begin{result}{3.63}[invertibility $\Longleftrightarrow$ injectivity and surjectivity]
A linear map is invertible if and only if it is injective and surjective.
\end{result}

\begin{result}{3.65}[injectivity is equivalent to surjectivity (if $\dim V = \dim W < \infty$)]
Suppose that $V$ and $W$ are finite-dimensional vector spaces, $\dim V = \dim W$, and $T \in \L(V, W)$. Then
$$
T \text{ is invertible } \Longleftrightarrow T \text{ is injective } \Longleftrightarrow T \text{ is surjective}.
$$
\end{result}

\begin{result}{3.68}[$ST = I \Longleftrightarrow TS = I$ (on vector spaces of the same dimension)]
Suppose $V$ and $W$ are finite-dimensional vector spaces of the same dimension, $S \in \L(V, W)$, and $T \in \L(W, V)$. Then $S T = I$ if and only if $T S = I$.
\end{result}

\begin{result}{3.70}[dimension shows whether vector spaces are isomorphic]
Two finite-dimensional vector spaces over $\F$ are isomorphic if and only if they have the same dimension.
\end{result}

\begin{result}{3.71}[$\L(V, W)$ and $\F^{m, n}$ are isomorphic]
Suppose $v_1, \ldots, v_n$ is a basis of $V$ and $w_1, \ldots, w_m$ is a basis of $W$. Then $M$ is an isomorphism between $\L(V, W)$ and $\F^{m, n}$.
\end{result}

\begin{result}{3.72}[$\dim \L(V, W) = (\dim V)(\dim W$]
Suppose $V$ and $W$ are finite-dimensional. Then $\L(V, W)$ is finite-dimensional and
$$
\dim \L(V, W) = (\dim V)(\dim W).
$$
\end{result}

\begin{result}{3.75}[$\M(T)_{\cdot, k} = \M(T v_k)$]
Suppose $T \in \L(V, W)$ and $v_1, \ldots, v_n$ is a basis of $V$ and $w_1, \ldots, w_m$ is a basis of $W$. Let $1 \le k \le n$. Then the $k^\text{th}$ column of $\M(T)$, which is denoted by $\M(T)_{\cdot, k}$, equals $\M(T v_k)$.
\end{result}

\begin{result}{3.76}[linear maps act like matrix multiplication]
Suppose $T \in \L(V, W)$ and $v \in V$. Suppose $v_1, \ldots, v_n$ is a basis of $V$ and $w_1 \ldots, w_m$ is a basis of $W$. Then
$$
\M(T v) = \M(T) \M(v) .
$$
\end{result}

\begin{result}{3.78}[dimension of range $T$ equals column rank of $\M(T)$]
Suppose $V$ and $W$ are finite-dimensional and $T \in \L(V, W)$. Then $\dim \range T$ equals the column rank of $\M(T)$.
\end{result}

\begin{result}{3.81}[matrix of product of linear maps]
Suppose $T \in \L(U, V)$ and $S \in \L(V, W)$. If $u_1, \ldots, u_m$ is a basis of $U$, $v_1, \ldots, v_n$ is a basis of $V$ and $w_1, \ldots, w_p$ is a basis for $W$, then
\begin{align*}
\M(ST, (u_1, \ldots, u_m), & (w_1, \ldots, w_p)) = \\
& \M(S, (v_1, \ldots, v_n), (w_1, \ldots, w_p)) \\
& \M(U, (u_1, \ldots, u_m), (v_1, \ldots, v_n)).
\end{align*}
\end{result}

\begin{result}{3.82}[matrix of identity operator with respect to two bases]
Suppose that $u_1, \ldots, u_n$ and $v_1, \ldots, v_n$ are bases of $V$. Then the matrices
$\M(I, (u_1, \ldots, u_n), (v_1, \ldots, v_n))$ and 
$\M(I, (v_1, \ldots, v_n), (u_1, \ldots, u_n))$
are invertible, and each is the inverse of the other.
\end{result}

\begin{result}{3.84}[change-of-basis formula]
Suppose $T \in \L(V)$. Suppose $u_1, \ldots, u_n$ and $v_1, \ldots, v_n$ are bases of $V$. Let
$A = \M(T, (u_1, \ldots, u_n))$ and $B = \M(T, (v_1, \ldots, v_n))$ and $C = \M(I, (u_1, \ldots, u_n), (v_1 \ldots, v_n))$. Then
$$
A = C^{-1} B C.
$$
\end{result}

\begin{result}{3.86}[matrix of inverse equals inverse of matrix]
Suppose that $v_1, \ldots, v_n$ is a basis of $V$ and $T \in \L(V)$ is invertible. Then $\M(T^{-1}) = (\M(T))^{-1}$, where both matrices are with respect to the basis $v_1, \ldots, v_n$.
\end{result}

\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Chapter 3.E}

\begin{definition}{3.87}[product of vector spaces]
Suppose $V_1, \ldots, V_m$ are vector spaces over $\F$.
\begin{enumerate}
\item The \defn{product} $V_1 \times \cdots \times V_m$ is defined by
$$
V_1 \times \cdots \times V_m = \{(v_1, \ldots, v_m) \setsep v_1 \in V_1, \ldots, v_m \in V_m \}.
$$
\item Addition on $V_1 \times \cdots \times V_m$ is defined by
$$
(u_1, \ldots, u_m) + (v_1, \ldots, v_m) = (u_1 + v_1, \ldots, u_m + v_m).
$$
\item Scalar multiplication on $V_1 \times \cdots \times V_m$ is defined by
$$
\lambda (v_1, \ldots, v_m) = (\lambda v_1, \ldots, \lambda v_m).
$$
\end{enumerate}
\end{definition}

\begin{notation}{3.95}[$v + U$]
Suppose $v \in V$ and $U \subseteq V$. Then $v + U$ is the subset of $V$ defined by
$$
v + U = \{ v + u \setsep u \in U \}
$$
\end{notation}

\begin{definition}{3.97}[translate]
For $v \in V$ and $U$ a subset of $V$, the set $v + U$ is said to be a \defn{translate} of $U$.
\end{definition}

\begin{definition}{3.99}[quotient space, $V / U$]
Suppose $U$ is a subspace of $V$. The \defn{quotient space} $V / U$ is the set of all translates of $U$. Thus
$$
V / U = \{ v + U \setsep v \in V \}.
$$
\end{definition}

\begin{definition}{3.102}[addition and scalar multiplication on $V / U$]
\label{def:quotientops}
Suppose $U$ is a subspace of $V$. The \defn{addition} and \defn{scalar multiplication} are defined on $V / U$ by
\begin{align*}
(v + U) + (w + U) &= (v + w) + U \\
\lambda (v + U) &= (\lambda v) + U
\end{align*}
for all $v, w \in V$ and all $\lambda \in \F$.
\end{definition}

\begin{definition}{3.104}[quotient map, $\pi$]
Suppose $U$ is a subspace of $V$. The \defn{quotient map} $\pi : V \to V / U$ is the linear map defined by
$$
\pi (v) = v + U
$$
for each $v \in V$.
\end{definition}

\begin{notation}{3.106}[$\widetilde{T}$]
Suppose $T \in \L (V, W)$. Define $\widetilde{T} : V / (\kernel T) \to W$ by
$$
\widetilde{T} (v + \kernel T) = T v.
$$
\end{notation}

\begin{notation}{\!\!}[composition of linear maps, $\circ$]
If $T \in \L(U, V)$ and $S \in \L(V, W)$, then the \defn{composition} notation $S \circ T$ is an alternative way of writing the product $S T$ of $S$ and $T$, as given in Definition \ref{def:mapproduct}.
\end{notation}

\newpage

\begin{result}{3.89}[product of vector spaces is a vector space]
Suppose $V_1, \ldots, V_n$ are vector spaces over $\F$. Then $V_1 \times \cdots \times V_m$ is a vector space over $\F$.
\end{result}

\begin{result}{3.92}[dimension of a product is the sum of dimensions]
Suppose $V_1, \ldots, V_m$ are finite-dimensional vector spaces. Then $V_1 \times \cdots \times V_m$ is finite-dimensional and
$$
\dim (V_1 \times \cdots \times V_m) = \dim V_1 + \cdots + \dim V_m.
$$
\end{result}

\begin{result}{3.93}[products and direct sums]
Suppose that $V_1, \ldots, V_m$ are subspaces of $V$. Define a linear map $\Gamma : V_1 \times \cdots \times V_m \to V_1 + \cdots + V_m$ by
$$
\Gamma (v_1, \ldots, v_m) = v_1 + \ldots + v_m.
$$
Then $V_1 + \cdots + V_m$ is a direct sum if and only if $\Gamma$ is injective.
\end{result}

\begin{result}{3.94}[a sum is a direct sum if and only if dimensions add up]
Suppose $V$ is finite-dimensional and $V_1, \ldots, V_m$ are subspaces of $V$. Then $V_1 + \cdots + V_m$ is a direct sum if and only if
$$
\dim (V_1 + \cdots + V_m) = \dim V_1 + \cdots + \dim V_m.
$$
\end{result}

\begin{result}{3.101}[two translates of a subspace are equal or disjoint]
Suppose $U$ is a subspace of $V$ and $v, w \in V$. Then
$$
v - w \in U \Longleftrightarrow v + U = w + U \Longleftrightarrow (v + U) \cap (w + U) \not= \emptyset.
$$
\end{result}

\begin{result}{3.103}[quotient space is a vector space]
Suppose $U$ is a subspace of $V$. Then $V / U$, with the operations of addition and scalar multiplication as defined in Definition \ref{def:quotientops}, is a vector space.
\end{result}

\begin{result}{3.105}[dimension of quotient space]
Suppose $V$ is finite-dimensional and $U$ is a subspace of $V$. Then
$$
\dim V / U = \dim U - \dim V.
$$
\end{result}

\begin{result}{3.107}[null space and range of $\widetilde{T}$]
Suppose $T \in \L (V, W)$. Then
\begin{enumerate}
\item[(a)] $\widetilde{T} \circ \pi = T$, where $\pi$ is the quotient map of $V$ onto $V / (\kernel T)$;
\item[(b)] $\widetilde{T}$ is injective;
\item[(c)] $\range \widetilde{T} = \range T$;
\item[(d)] $V / (\kernel T)$ and $\range T$ are isomorphic vector spaces.
\end{enumerate}
\end{result}

% Exercise 18
\begin{result}{Ex. 3E, 18}[Direct sum of a quotient]
Suppose $U$ is a subspace of $V$ such that $V / U$ is finite-dimensional. Then there exists a finite-dimensional subspace $W$ of $V$ such that $\dim W = \dim V / U$ and $V = U \oplus W$.
\end{result}

\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Chapter 3.F}

\begin{definition}{3.108}[linear functional]
A \defn{linear functional} on $V$ is a linear map from $V$ to $\F$. In other words, a linear functional is an element of $\L(V, \F)$.
\end{definition}

\begin{definition}{3.110}[dual space $V'$]
The \defn{dual space} of $V$, denoted by $V'$, is the vector space of all linear functionals on $V$. In other words, $V' = \L(V, \F)$.
\end{definition}

\begin{definition}{3.112}[dual basis]
If $v_1, \ldots, v_n$ is a basis of $V$, then the \defn{dual basis} of $v_1, \ldots, v_n$ is the list $\varphi_1, \ldots, \varphi_n$ of elements of $V'$, where each $\varphi_j$ is the linear functional on $V$ such that
$$
\varphi_j(v_k) =
\begin{cases}
1 & \text{if } k = j, \\
0 & \text{if } k \not= j.
\end{cases}
$$
\end{definition}

\begin{definition}{3.118}[dual map, $T'$]
Suppose $T \in \L(V, W)$. The \defn{dual map} of $T$ is the linear map $T' \in \L(W', V')$ defined for each $\varphi \in W'$ by
$$
T'(\varphi) = \varphi \circ T.
$$
\end{definition}

\begin{definition}{3.121}[annihilator, $U^0$]
For $U \subseteq V$, the \defn{annihilator} of $U$, denoted by $U^0$, is defined by
$$
U^0 = \{ \varphi \in V' \setsep \varphi(u) = 0 \text{ for all } u \in U \}.
$$
\end{definition}

\newpage

\begin{result}{3.111}[$\dim V' = \dim V$]
Suppose $V$ is finite-dimensional. Then $V'$ is also finite-dimensional and
$$
\dim V' = \dim V.
$$
\end{result}

\begin{result}{3.114}[dual basis gives coefficients for linear combination]
Suppose $v_1, \ldots, v_n$ is a basis of $V$ and $\varphi_i, \ldots, \varphi_n$ is the dual basis. Then for each $v \in V$
$$
v = \varphi_1(v) v_1 + \cdots + \varphi_n(v) v_n
$$
\end{result}

\begin{result}{3.116}[dual basis is a basis of the dual space]
Suppose $V$ is finite-dimensional. Then the dual basis of a basis of $V$ is a basis of $V'$.
\end{result}

\begin{result}{3.120}[algebraic properties of dual maps]
Suppose $T \in \L(V, W)$. Then
\begin{enumerate}
\item[(a)] $(S + T)' = S' + T'$ for all $S \in \L(V, W)$;
\item[(b)] $(\lambda T)' = \lambda T'$ for all $\lambda \in \F$;
\item[(c)] $(ST)' = T' S'$ for all $S \in \L(W, U)$.
\end{enumerate}
\end{result}

\begin{result}{3.124}[the annihilator is a subspace]
Suppose $U \subseteq V$. Then $U^0$ is a subspace of $V'$.
\end{result}

\begin{result}{3.125}[dimension of the annihilator]
If $V$ is finite-dimensional and $U$ is a subspace of $V$ then
$$
\dim U^0 = \dim V - \dim U.
$$
\end{result}

\begin{result}{3.127}[condition for the annihilator to equal $\{ 0 \}$ or the whole space]
If $V$ is finite-dimensional and $U$ is a subspace of $V$ then
\begin{enumerate}
\item[(a)] $U^0 = \{ 0 \} \Longleftrightarrow U = V$;
\item[(b)] $U^0 = V' \Longleftrightarrow U = \{ 0 \}$.
\end{enumerate}
\end{result}

\begin{result}{3.128}[the null space of $T'$]
Suppose $T \in \L(V, W)$. Then
\begin{enumerate}
\item[(a)] $\kernel T' = (\range T)^0$.
\end{enumerate}
Suppose further that $V$ and $W$ are finite-dimensional. Then
\begin{enumerate}
\item[(b)] $\dim \kernel T' = \dim \kernel T + \dim W - \dim W$.
\end{enumerate}
\end{result}

\begin{result}{3.129}[$T$ surjective is equivalent to $T'$ injective]
If $V$ and $W$ are finite-dimensional and $T \in \L(V, W)$ then
$$
T \text{ is surjective } \Longleftrightarrow T' \text{ is injective}.
$$
\end{result}

\begin{result}{3.130}[the range of $T'$]
If $V$ and $W$ are finite-dimensional and $T \in \L(V, W)$ then
\begin{enumerate}
\item[(a)] $\dim \range T' = \dim \range T$;
\item[(b)] $\range T' = (\kernel T)^0$.
\end{enumerate}
\end{result}

\begin{result}{3.131}[$T$ injective is equivalent to $T'$ surjective]
If $V$ and $W$ are finite-dimensional and $T \in \L(V, W)$ then
$$
T \text{ is injective } \Longleftrightarrow T' \text{ is surjective}.
$$
\end{result}

\begin{result}{3.132}[matrix of $T'$ is transpose of matrix of $T$]
If $V$ and $W$ are finite-dimensional and $T \in \L(V, W)$ then
$$
\M(T') = \transpose{(\M(T))}.
$$
\end{result}

\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Chapter 4}

\begin{definition}{4.1}[real part, $\Real{z}$, imaginary part, $\Imag{z}$]
Suppose $z = a + bi$, where $a$ and $b$ are real numbers.
\begin{enumerate}
\item The \defn{real part} of $z$, denoted $\Real{z}$, is defined by $\Real{z} = a$.
\item The \defn{imaginary part} of $z$, denoted by $\Imag{z}$, is defined by $\Imag{z} = b$.
\end{enumerate}
\end{definition}

\begin{definition}{4.2}[complex conjugate, $\bar{z}$, absolute value, $|z|$]
Suppose $z \in \C$.
\begin{enumerate}
\item The \defn{complex conjugate} of $z \in \C$, denoted by $\bar{z}$, is defined by
$$
\bar{z} = \Real{z} - (\Imag{z}) i.
$$
\item The \defn{absolute value} of a complex number $z$, denoted by $|z|$, is defined by
$$
|z| = \sqrt{(\Real{z})^2 + (\Imag{z})^2}.
$$
\end{enumerate}
\end{definition}

\begin{property}{4.4}[properties of complex numbers]
Suppose $w, z \in \C$. Then the following equalities and inequalities hold.

\defn{sum of $z$ and $\bar{z}$}
\begin{forceindent}
$z + \bar{z} = 2 \Real{z}$.
\end{forceindent}

\defn{difference of $z$ and $\bar{z}$}
\begin{forceindent}
$z - \bar{z} = 2 (\Imag{z}) i$.
\end{forceindent}

\defn{product of $z$ and $\bar{z}$}
\begin{forceindent}
$z \bar{z} = |z|^2$.
\end{forceindent}

\defn{additivity and multiplicativity of complex conjugate}
\begin{forceindent}
$\overline{w + z} = \bar{w} + \bar{z}$ and $\overline{w z} = \bar{w} \bar{z}$.
\end{forceindent}

\defn{double complex conjugate}
\begin{forceindent}
$\bar{\bar{z}} = z$.
\end{forceindent}

\defn{real and imaginary parts are bounded by $|z|$}
\begin{forceindent}
$|\Real{z}| \le |z|$ and $|\Imag{z}| \le |z|$.
\end{forceindent}

\defn{absolute value of the complex conjugate}
\begin{forceindent}
$|\bar{z}| = |z|$.
\end{forceindent}

\defn{multiplicativity of absolute value}
\begin{forceindent}
$|w z| = |w| |z|$.
\end{forceindent}

\defn{triangle inequality}
\begin{forceindent}
$|w + z| \le |w| + |z|$.
\end{forceindent}
\end{property}

\begin{definition}{4.5}[zero of a polynomial]
A number $\lambda \in \F$ is called a \defn{zero} (or \defn{root}) of a polynomial $p \in \mathscr{P}(\F)$ if
$$
p(\lambda) = 0 .
$$
\end{definition}

\newpage

\begin{result}{4.6}[each zero of a polynomial corresponds to a degree-one factor]
Suppose $m$ is a positive integer and $p \in \mathscr{P}(\F)$ is a polynomial of degree $m$. Suppose $\lambda \in \F$. Then $p(\lambda) = 0$ if and only if there exists a polynomial $q \in \mathscr{P}(\F)$ of degree $m - 1$ such that
$$
p(z) = (z - \lambda) q(z)
$$
for every $z \in \F$.
\end{result}

\begin{result}{4.8}[degree $m$ implies at most $m$ zeros]
Suppose $m$ is a positive integer and $p \in \mathscr{P}(\F)$ is a polynomial of degree $m$. Then $p$ has at most $m$ zeros in $\F$.
\end{result}

\begin{result}{4.9}[division algorithm for polynomials]
Suppose that $p, s \in \mathscr{P}(\F)$, with $s \not= 0$. Then there exist unique polynomials $q, r \in \mathscr{P}(\F)$ such that
$$
p = s q + r
$$
and $\deg{r} < \deg{s}$.
\end{result}

\begin{result}{4.12}[fundamental theorem of algebra, first version]
Every nonconstant polynomial with complex coefficients has a zero in $\C$.
\end{result}

\begin{result}{4.13}[fundamental theorem of algebra, second version]
If $p \in \mathcal{P}(\C)$ is a nonconstant polynomial, then $p$ has a unique factorisation (except for the order of the factors) of the form
$$
p(z) = c (z - \lambda_1) \cdots (z - \lambda_m) ,
$$
where $c, \lambda_1, \ldots, \lambda_m \in \C$.
\end{result}

\begin{result}{4.14}[polynomials with real coefficients have nonreal zeros in pairs]
Suppose $p \in \mathcal{P}(\C)$ is a polynomial with real coefficients. If $\lambda \in \C$ is a zero of $p$, then so is $\bar{\lambda}$.
\end{result}

\begin{result}{4.15}[factorisation of a quadratic polynomial]
Suppose $b, c \in \R$. Then there is a polynomial factorisation of the form
$$
x^2 + b x + c = (x - \lambda_1) (x - \lambda_2)
$$
with $\lambda_1, \lambda_2 \in \R$ if and only if $b^2 \ge 4 c$.
\end{result}

\begin{result}{4.16}[factorisation of a polynomial over $\R$]
Suppose $p \in \mathcal{P}(\R)$ is a nonconstant polynomial. Then $p$ has a unique factorisation (except for the order of the factors) of the form
$$
p(x) = c (x - \lambda_1) \cdots (x - \lambda_m) (x^2 + b_1 x + c_1) \cdots (x^2 + b_M x + c_M ) .
$$
where $c, \lambda_1, \ldots, \lambda_m, b_1, \ldots, b_M, c_1, \ldots, c_M \in \R$ with $b_k^2 < 4 c_k$ for each $k$.
\end{result}

\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Chapter 5.A}

\begin{definition}{5.1}[operator]
A linear map from a vector space to itself is called an $operator$.
\end{definition}

\begin{definition}{5.2}[invariant subspace]
Suppose $T \in \L(V)$. A subspace $U$ of $V$ is called \defn{invariant} under $T$ if $T u \in U$ for every $u \in U$.
\end{definition}

\begin{definition}{5.5}[eigenvalue]
Suppose $T \in \L(V)$. A number $\lambda \in \F$ is called an \defn{eigenvalue} of $T$ if there exists $v \in V$ such that $v \not= 0$ and $T v = \lambda v$.
\end{definition}

\begin{definition}{5.8}[eigenvector]
Suppose $T \in \L(V)$ and $\lambda \in \F$ is an eigenvalue of $T$. A vector $v \in V$ is called an \defn{eigenvector} of $T$ corresponding to $\lambda$ if $v \not= 0$ and $T v = \lambda v$.
\end{definition}

\begin{notation}{5.13}[$T^m$]
Suppose $T \in \L(V)$ and $m$ is a positive integer.
\begin{enumerate}
\item $T^m \in \L(V)$ is defined by $T^m = \underbrace{T \cdots T}_{\text{$m$ times}}$.
\item $T^0$ is defined to be the identity operator $I$ on $V$.
\item If $T$ is invertible with inverse $T^{-1}$, then $T^{-m} \in \L(V)$ is defined by
$$
T^{-m} = (T^{-1})^m .
$$
\end{enumerate}
\end{notation}

\begin{notation}{5.14}[$p(T)$]
Suppose $T \in \L(V)$ and $p \in \mathcal{P}(\F)$ is a polynomial given by
$$
p(z) = a_0 + a_1 z + a_2 z^2 + \cdots + a_m z^m
$$
for all $z \in \F$. Then $p(T)$ is the operator $V$ defined by
$$
p(T) = a_0 I + a_1 T + a_2 T^2 + \cdots + a_m T^m .
$$
\end{notation}

\begin{definition}{5.16}[product of polynomials]
If $p, q \in \mathcal{P}(\F)$, then $pq \in \mathcal{P}(\F)$ is the polynomial defined by
$$
(p q)(z) = p(z) q(z)
$$
for all $z \in \F$.
\end{definition}

\newpage

\begin{result}{5.7}[equivalent conditions to be an eigenvalue]
Suppose $V$ is finite-dimensional, $T \in \L(V)$, and $\lambda \in F$. Then the following are equivalent.
\begin{enumerate}
\item[(a)] $\lambda$ is an eigenvalue of $T$.
\item[(b)] $T - \lambda I$ is not injective.
\item[(c)] $T - \lambda I$ is not surjective.
\item[(d)] $T - \lambda I$ is not invertible.
\end{enumerate}
\end{result}

\begin{result}{5.11}[linearly independent eigenvectors]
Suppose $T \in \L(V)$. Then every list of eigenvectors of $T$ corresponding to distinct eigenvalues of $T$ is linearly independent.
\end{result}

\begin{result}{5.12}[operator cannot have more eigenvalues than dimension of vector space]
Suppose $V$ is finite-dimensional. Then each operator on $V$ has at most $\dim{V}$ distinct eigenvalues.
\end{result}

\begin{result}{5.17}[multiplicative properties]
Suppose $p, q \in \mathcal{P}(\F)$ and $T \in \L(V)$. Then
\begin{enumerate}
\item[(a)] $(p q)(T) = p(T) q(T)$;
\item[(b)] $p(T) q(T) = q(T) p(T)$.
\end{enumerate}
\end{result}

\begin{result}{5.18}[null space and range of $p(T)$ are invariant under $T$]
Suppose $T \in \L(V)$ and $p \in \mathcal{P}(\F)$. Then $\kernel{p(T)}$ and $\range{p(T)}$ are invariant under $T$.
\end{result}

\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Chapter 5.B}

\begin{definition}{5.21}[monic polynomial]
A \defn{monic polynomial} is a polynomial whose highest-degree coefficient equals 1.
\end{definition}

\begin{definition}{5.24}[minimal polynomial]
Suppose $V$ is finite-dimensional and $T \in \L(V)$. Then the \defn{minimial polynomial} of $T$ is the unique monic polynomial $p \in \mathcal{P}(\F)$ of smallest degree such that $p(T) = 0$.
\end{definition}

\newpage

\begin{result}{5.19}[existence of eigenvalues]
Every operator on a finite-dimensional nonzero complex vector space has an eigenvalue.
\end{result}

\begin{result}{5.22}[existence, uniqueness, and degree of minimal polynomial]
Suppose $V$ is finite-dimensional and $T \in \L(V)$. Then there is a unique monic polynomial $p \in \mathcal{P}(\F)$ of smallest degree such that $p(T) = 0$. Furthermore, $\deg p \le \dim V$.
\end{result}

\begin{result}{5.27}[eigenvalues are the zeros of the minimal polynomial]
Suppose $V$ is finite-dimensional and $T \in \L(V)$,
\begin{enumerate}
\item[(a)] The zeros of the minimal polynomial of $T$ are the eigenvalues of $T$.
\item[(b)] If $V$ is a complex vector space, then the minimal polynomial of $T$ has the form
$$
(z - \lambda_1) \cdots (z - \lambda_m),
$$
where $\lambda_1, \ldots, \lambda_m$ is a list of all eigenvalues of $T$, possibly with repetitions.
\end{enumerate}
\end{result}

\begin{result}{5.29}[$q(T) = 0 \Longleftrightarrow q$ is a polynomial multiple of the minimal polynomial]
Suppose $V$ is finite-dimensional, $T \in \L(V)$, and $q \in \mathcal{P}(\F)$. Then $q(T) = 0$ if and only if $q$ is a polynomial multiple of the minimal polynomial $T$.
\end{result}

\begin{result}{5.31}[minimal polynomial of a restriction operator]
Suppose $V$ is finite-dimensional, $T \in \L(V)$, and $U$ is a subspace of $V$ that is invariant under $T$. Then the minimal polynomial of $T$ is a polynomial multiple of the minimal polynomial of $T|_U$.
\end{result}

\begin{result}{5.32}[$T$ not invertible $\Longleftrightarrow$ constant term of minimal polynomial of $T$ is 0]
Suppose $V$ is finite-dimensional and $T \in \L(V)$. Then $T$ is not invertible if and only if the constant term of the minimal polynomial of $T$ is 0.
\end{result}

\begin{result}{5.33}[even-dimensional null space]
Suppose $\F = \R$ and $V$ is finite-dimensional. Suppose also that $T \in \L(V)$ and $b, c \in \R$ with $b^2 < 4c$. Then $\dim \null(T^2 + bT +cI)$ is an even number.
\end{result}

\begin{result}{5.34}[operators on odd-dimensional vector spaces have eigenvalues]
Every operator of an odd-dimensional vector space has an eigenvalue.
\end{result}

\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Chapter 5.C}

\begin{definition}{5.35}[matrix of an operator, $\M(T)$]
Suppose $T \in \L(V)$. The \defn{matrix of $T$} with respect to a basis $v_1, \ldots, v_n$ of $V$ is the $n$-by-$n$ matrix
$$
\M(T) = 
\begin{pmatrix}
A_{1, 1} & \cdots & A_{1, n} \\
\vdots & & \vdots \\
A_{n, 1} & \cdots & A_{n, n} \\
\end{pmatrix}
$$
whose entries $A_{j, k}$ are defined by
$$
T v_k = A_{1, k} v_1 + \cdots + A_{n, k} v_n .
$$
The notation $\M(T, (v, 1, \ldots, v_n))$ is used if the basis is not clear from the context.
\end{definition}

\begin{definition}{5.37}[diagonal of a matrix]
The \defn{diagonal} of a square matrix consists of the entries on the line from the upper left corner to the bottom right corner.
\end{definition}

\begin{definition}{5.38}[upper-triangular matrix]
A square matrix is called \defn{upper triangular} if all entries below the diagonal are 0.
\end{definition}

\newpage

\begin{result}{5.39}[conditions for upper-triangular matrix]
Suppose $T \in \L(V)$ and $v_1 \ldots, v_n$ is a basis for $V$. Then the following are equivalent.
\begin{enumerate}
\item[(a)] The matrix of $T$ with respect to $v_1, \ldots, v_n$ is upper triangular.
\item[(b)] $\vspan(v_1, \ldots, v_k)$ is invariant under $T$ for each $k = 1, \ldots, n$.
\item[(c)] $T v_k \in \vspan(v_1, \ldots, v_k)$ for each $k = 1, \ldots, n$.
\end{enumerate}
\end{result}

\begin{result}{5.40}[equation satisfied by operator with upper-triangular matrix]
Suppose $T \in \L(V)$ and $V$ has a basis with respect to which $T$ has an upper-triangular matrix with diagonal entries $\lambda_1, \ldots, \lambda_n$. Then
$$
(T - \lambda_1 I) \cdots (T - \lambda_n I) = 0.
$$
\end{result}

\begin{result}{5.41}[determination of eigenvalues from upper-triangular matrix]
Suppose $T \in \L(V)$ has an upper-triangular matrix with respect to some basis of $V$. Then the eigenvalues of $T$ are precisely the entries on the diagonal of that upper-triangular matrix.
\end{result}

\begin{result}{5.44}[necessary and sufficient condition to have an upper-triangular matrix]
Suppose $V$ is finite-dimensional and $T \in \L(V)$. Then $T$ has an upper-triangular matrix with respect to some basis of $V$ if and only if the minimal polynomial of $T$ equals $(z - \lambda_1) \cdots (z - \lambda_m)$ for some $\lambda_1, \ldots, \lambda_m \in \F$.
\end{result}

\begin{result}{5.47}[if $\F = \C$, then every operator on $V$ has an upper triangular matrix]
Suppose $V$ is a finite-dimensional complex vector space and $T \in \L(V)$. Then $T$ has an upper-triangular matrix with respect to some basis of $V$.
\end{result}

\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Chapter 5.D}

\begin{definition}{5.48}[diagnoal matrix]
A \defn{diagonal matrix} is a square matrix that is 0 everywhere except possibly on the diagonal.
\end{definition}

\begin{definition}{5.50}[diagonalizable]
An operator $V$ is called \defn{diagonizable} if the operator has a diagonal matrix with respect to some basis $V$.
\end{definition}

\begin{definition}{5.52}[eigenspace, $E(\lambda, T)$]
Suppose $T \in \L(V)$ and $\lambda \in \F$. The \defn{eigenspace} of $T$ corresponding to $\lambda$ is the subspace $E(\lambda, T)$ of $V$ defined by
$$
E(\lambda, T) = \kernel(T - \lambda I) = \{ v \in V \setsep Tv = \lambda v \}.
$$
Hence $E(\lambda, T)$ is the set of all eigenvectors of $T$ corresponding to $\lambda$, along with the 0 vector.
\end{definition}

\begin{definition}{5.66}[Gershgorin disks]
Suppose $T \in \L(V)$ and $v_1, \ldots, v_n$ is a basis of $V$. Let $A$ denote the matrix of $T$ with respect to this basis. A \defn{Gershgorin disk} of $T$ with respect to the basis $v_1, \ldots, v_n$ is a set of the form
$$
\left\{ z \in \F \setsep |z - A_{j, j}| \le \smash{\sum_{\substack{k = 1 \\ k \not= j}}^n} \vphantom{\sum^n} |A_{j, k}| \right\},
$$
where $j \in \{1, \ldots, n\}$.
\end{definition}

\newpage

\begin{result}{5.54}[sum of eigenspaces is a direct sum]
Suppose $T \in \L(V)$ and $\lambda_1, \ldots, \lambda_m$ are distinct eigenvalues of $T$. Then
$$
E(\lambda_1, T) + \cdots + E(\lambda_m, T)
$$
is a direct sum. Furthermore, if $V$ is finite-dimensional, then
$$
\dim E(\lambda_1, T) + \cdots + \dim E(\lambda_m, T) \le \dim V .
$$
\end{result}

\begin{result}{5.55}[conditions equivalent to diagonalizability]
Suppose $V$ is finite-dimensional and $T \in \L(V)$. Let $\lambda_1, \ldots, \lambda_m$ denote the distinct eigenvalues of $T$. Then the following are equivalent.
\begin{enumerate}
\item[(a)] $T$ is diagonalizable.
\item[(b)] $V$ has a basis consisting of eigenvectors of $T$.
\item[(c)] $V = E(\lambda_1, T) \oplus \cdots \oplus E(\lambda_m, T)$.
\item[(d)] $\dim V = \dim E(\lambda_1, T) + \cdots + \dim E(\lambda_m, T)$.
\end{enumerate}
\end{result}

\begin{result}{5.58}[enough eigenvalues implies diagonalizability]
Suppose $V$ is finite-dimensional and $T \in \L(V)$ has $\dim V$ distinct eigenvalues. Then $T$ is diagonalizable.
\end{result}

\begin{result}{5.62}[necessary and sufficient condition for diagonalizability]
Suppose $V$ is finite-dimensional and $T \in \L(V)$. Then $T$ is diagonalizable if and only if the minimal polynomial of $T$ equals $(z - \lambda_1) \cdots (z - \lambda_m)$ for some list of distinct numbers $\lambda_1, \ldots, \lambda_m \in \F$.
\end{result}

\begin{result}{5.65}[restriction of diagonalizable operator to invariant subspace]
Suppose $T \in \L(V)$ is diagonalizable and $U$ is a subspace of $V$ that is invariant under $T$. Then $T|_U$ is a diagonalizable operator on $U$.
\end{result}

\begin{theorem}{5.67}[Gershgorin disk theorem]
Suppose $T \in \L(V)$ and $v_1, \ldots, v_n$ is a basis of $V$. Then each eigenvalue of $T$ is contained in some Gershgorin disk of $T$ with respect to the basis $v_1, \ldots, v_n$.
\end{theorem}

\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Chapter 5.E}

\begin{definition}{5.71}[commute] \enumfix
\begin{enumerate}
\item Two operators $S$ and $T$ on the same vector space \defn{commute} if $ST = TS$.
\item Two square matrices $A$ and $B$ of the same size \defn{commute} if $AB = BA$.
\end{enumerate}
\end{definition}

\newpage

\begin{result}{5.74}[commuting operators correspond to commuting matrices]
Suppose $S, T \in \L(V)$ and $v_1, \ldots, v_n$ is a basis of $V$. Then $S$ and $T$ commute if and only if $\M(S, (v_1, \ldots, v_n))$ and $\M(T, (v_1, \ldots, v_n))$ commute.
\end{result}

\begin{result}{5.75}[eigenspace is invariant under commuting operator]
Suppose $S, T \in \L(V)$ commute and $\lambda \in \F$. Then $E(\lambda, S)$ is invariant under $T$.
\end{result}

\begin{result}{5.76}[simultaneous diagonalizability $\Longleftrightarrow$ commutativity]
Two diagonalizable operators on the same vector space have diagonal matrices with respect to the same basis if and only if the two operators commute.
\end{result}

\begin{result}{5.78}[common eigenvector for commuting operators]
Every pair of commuting operators on a finite-dimensional nonzero complex vector space has a common eigenvector.
\end{result}

\begin{result}{5.80}[commuting operators are simultaneously upper triangularizable]
Suppose $V$ is a finite-dimensional complex vector space and $S, T$ are commuting operators on $V$. Then there is a basis of $V$ with respect to which both $S$ and $T$ have upper-triangular matrices.
\end{result}

\begin{result}{5.81}[eigenvalues of sum and product of commuting operators]
Suppose $V$ is a finite-dimensional complex vector space and $S, T$ are commuting operators on $V$. Then
\begin{enumerate}
\item every eigenvalue of $S + T$ is an eigenvalue of $S$ plus an eigenvalue of $T$,
\item every eigenvalue of $ST$ is an eigenvalue of $S$ times an eigenvalue of $T$.
\end{enumerate}
\end{result}

\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Chapter 6.A}

\begin{definition}{6.1}[dot product]
For $x, y \in \R^n$, the \defn{dot product} of $x$ and $y$, denoted $x \cdot y$, is defined by
$$
x \cdot y = x_1 y_1 + \cdots + x_n y_n,
$$
where $x = (x_1, \ldots, x_n)$ and $y = (y_1, \ldots, y_n)$.
\end{definition}

\begin{notation}{6.1\textonehalf}[complex nonnegative]
For $\lambda \in \C$, the notation $\lambda \ge 0$ means $\lambda$ is real and nonnegative.
\end{notation}

\begin{definition}{6.2}[inner product]
An \defn{inner product} on $V$ is a function that takes each ordered pair $(u, v)$ of elements of $V$ to a number $\inner{u}{v} \in \F$ and has the following properties.

\defn{positivity}
\begin{forceindent}
$\inner{v}{v} \ge 0$ for all $v \in V$.
\end{forceindent}

\defn{definiteness}
\begin{forceindent}
$\inner{v}{v} = 0$ if and only if $v = 0$.
\end{forceindent}

\defn{additivity in first slot}
\begin{forceindent}
$\inner{u + v}{w} = \inner{u}{w} + \inner{v}{w}$ for all $u, v, w \in V$.
\end{forceindent}

\defn{homogeneity in first slot}
\begin{forceindent}
$\inner{\lambda u}{v} = \lambda \inner{u}{v}$ for all $\lambda \in \F$ and all $u, v \in V$.
\end{forceindent}

\defn{conjugate symmetry}
\begin{forceindent}
$\inner{u}{v} = \overline{\inner{v}{u}}$ for all $u, v \in V$.
\end{forceindent}
\end{definition}

\begin{definition}{6.4}[inner product space]
An \defn{inner product space} is a vector space $V$ along with an inner product $V$.
\end{definition}

\begin{notation}{6.5}[$V, W$]
For chapters 6 and 7, $V$ and $W$ denote inner product spaces over $F$.
\end{notation}

\begin{definition}{6.7}[norm, $\norm{v}$]
For $v \in V$, the \defn{norm}, denoted by $\norm{v}$, is defined by
$$
\norm{v} = \sqrt{\inner{v}{v}} .
$$
\end{definition}

\begin{definition}{6.10}[orthogonal]
Two vectors $u, v \in V$ are called \defn{orthogonal} if $\inner{u}{v} = 0$.
\end{definition}

\newpage

\begin{result}{6.6}[basic properties of an inner product] \enumfix
\begin{enumerate}
\item[(a)] For each fixed $v \in V$, the function that takes $u \in V$ to $\inner{u}{v}$ is a linear map from $V$ to $\F$.
\item[(b)] $\inner{0}{v} = 0$ for every $v \in V$.
\item[(c)] $\inner{v}{0} = 0$ for every $v \in V$.
\item[(d)] $\inner{u}{v + w} = \inner{u}{v} + \inner{u}{w}$ for all $u, v, w \in V$.
\item[(e)] $\inner{u}{\lambda v} = \bar\lambda \inner{u}{v}$ for all $\lambda \in \F$ and all $u, v \in V$.
\end{enumerate}
\end{result}

\begin{result}{6.9}[basic properties of the norm]
Suppose $v \in V$.
\begin{enumerate}
\item[(a)] $\norm{v} = 0$ if and only if $v = 0$.
\item[(b)] $\norm{\lambda v} = |\lambda| \norm{v}$ for all $\lambda \in \F$.
\end{enumerate}
\end{result}

% Exercise 15
\begin{result}{Ex. 6A, 15}[angle between vectors in $\R^2$]
If $u, v \in \R^2$ are non-zero then
$$
\inner{u}{v} = \norm{u} \norm{v} \cos \theta,
$$
where $\theta$ is the angle between $u$ and $v$.
\end{result}

\begin{result}{6.11}[orthogonality and 0] \enumfix
\begin{enumerate}
\item[(a)] 0 is orthogonal to every vector in $V$.
\item[(b)] 0 is the only vector in $V$ that is orthogonal to itself.
\end{enumerate}
\end{result}

\begin{theorem}{6.12}[Pythagorean theorem]
Suppose $u, v \in V$. If $u$ and $v$ are orthogonal, then
$$
\norm{u + v}^2 = \norm{u}^2 + \norm{v}^2 .
$$
\end{theorem}

\begin{result}{6.13}[an orthogonal decomposition]
Suppose $u, v \in V$, with $v \not= 0$. Set $c = \frac{\inner{u}{v}}{\norm{v}^2}$ and $w = u - \frac{\inner{u}{v}}{\norm{v}^2} v$. Then
$$
u = cv + w \quad \text{and} \quad \inner{w}{v} = 0.
$$
\end{result}

\begin{theorem}{6.14}[Cauchy-Schwarz inequality]
Suppose $u, v \in V$. Then
$$
|\inner{u}{v}| \le \norm{u}\norm{v} .
$$
This inequality is an equality if and only if one of $u, v$ is a scalar multiple of the other.
\end{theorem}

\begin{theorem}{6.17}[triangle inequality]
Suppose $u, v \in V$. Then
$$
\norm{u + v} \le \norm{u} + \norm{v} .
$$
This inequality is an equality if and only if one of $u, v$ is a nonnegative real multiple of the other.
\end{theorem}

% Exercise 20
\begin{result}{Ex. 6A, 20}[reverse triangle inequality]
If $u, v \in V$, then
$$
\left| \vphantom{\sum} \norm{u} - \norm{v} \vphantom{\sum} \right| \le \norm{u - v} .
$$
\end{result}

\begin{result}{6.21}[parallelogram inequality]
Suppose $u, v \in V$. Then
$$
\norm{u + v}^2 + \norm{u - v}^2 = 2(\norm{u}^2 + \norm{v}^2 ) .
$$
\end{result}

\end{document}
